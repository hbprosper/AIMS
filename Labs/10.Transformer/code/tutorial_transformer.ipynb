{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hbprosper/AIMS/blob/main/Labs/10.Transformer/code/tutorial_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DlGeQP8NkVv"
      },
      "source": [
        "# Tutorial: Transformer Neural Networks (TNN)\n",
        "> Created Aug. 2024 for the FSU Course: *Machine Learning in Physics* <br>\n",
        "> Update Nov 1 2025: Improve explanations<br>\n",
        "> Update Nov 2 2025: Restructure code<br>\n",
        "> H. B. Prosper<br>\n",
        "\n",
        "Based on project by former FSU student Alex Judge<br>\n",
        "Florida State University, Spring 2023 (closely follows the Annotated Transformer[1])<br>\n",
        "Updated: July 4, 2023 for Terascale 2023, DESY, Hamburg, Germany<br>\n",
        "Updated: March 31, 2024 HBP: load all data onto computational device<br>\n",
        "Updated: November 19, 2024 HBP: for *Machine Learning in Physics course*<br>\n",
        "Updated: October 27, 2025 HBP: for *Machine Learning in Physics course*\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This tutorial describes a sequence to sequence (**seq2seq**) neural network, called the **transformer**[1], which translates one sequence of tokens to another. The tutorial follows closely the excellent description in the Annotated Transformer[2].\n",
        "\n",
        "In natural language translation, a word, for example $\\texttt{the}$, or part of a word, for example $\\texttt{ly}$, could be a token. In symbolic mathematics, a token might be a mathematical function, e.g. $\\texttt{sin}$. The set of tokens forms a **vocabulary** from which sequences of tokens are constructed. Typically, the vocabulary of the input sequences (the source) differs from that of the output sequences (the target). This makes sense. If one is translating from English to French, it makes sense to have different vocabularies for the two languages.\n",
        "\n",
        "The seq2seq model\n",
        "consists of three parts:\n",
        "\n",
        "  1. The embedding layers: encodes the tokens and their relative positions within sequences. An input (i.e., source) sequence of tokens is thus mapped to a point cloud in a vector space.\n",
        "  1. The transformer layers[1]: implements the syntactic and semantic analysis.\n",
        "  1. The output layer: computes weights, one for every possible token in the output vocabulary of tokens, which are converted to probabilistic predictions for the next token in the output sequence given the input sequence and the current output sequence.\n",
        "\n",
        "__Tensor Convention__\n",
        "We follow the convention used in the Annotated Transformer[2] in which the batch is the first dimension in all tensors.\n",
        "\n",
        "## Sequence to Sequence Model\n",
        "\n",
        "### Introduction\n",
        "A transformer-based seq2seq model comprises an **encoder** and a **decoder**. The encoder embeds every token in the source sequence $\\boldsymbol{x}$ together with its ordinal value  in a vector space. The vectors are processed with a chain of algorithms called **attention** and the transformed vectors together with the current target sequence $\\boldsymbol{t}$ or current predicted output sequence $\\boldsymbol{y}$ are sent to the decoder, which embeds the targets in the same vector space. The target vectors are likewise processed with a chain of attention algorithms, while the target vectors and those from the encoder are processed with another attention algorithm. Finally, the decoder assigns a weight to every token in the target vocabulary. Using a greedy strategy, one chooses the next output token to be the one with the largest weight, that is, the most probable token. The model is **autoregressive**: the predicted token is appended to the existing predicted output sequence and the model is called again with the same source and the updated output. The procedure repeats until either the maximum output sequence length is reached or the end-of-sequence (EOS) token is predicted as the most probable token.\n",
        "\n",
        "\n",
        "### Attention\n",
        "\n",
        "When we translate from one sequence of symbols to another sequence of symbols, for example from one natural language to another,  the meaning of the sequences is encoded in the symbols, their relative order, and the degree to which a given symbol is related to the other symbols. Consider the phrases \"the white house\" and \"la maison blanche\". In order to obtain a correct translation it is important for the model to encode the fact that \"la\" and \"maison\" are strongly related, while \"the\" and \"house\" are less so. It is also important for the model to encode the strong relationship between \"the\" and \"la\", between \"house\" and \"maison\", and between \"white\" and \"blanche\". That is, the model needs to *pay attention to* grammatical and semantic facts. At least as far as we can tell that's what humans do.\n",
        "\n",
        "The need for the model to pay attention to relevant linguistic facts is the basis of the so-called [attention mechanism](https://nlp.seas.harvard.edu/annotated-transformer/). In the encoding stage, the model associates a vector to every token that tries to capture the strength of a token's relationship to other tokens. Since this association mechanism operates within the same sequence (that is, within the same point cloud in the vector space in which the sequence is embedded) it is referred to as **self attention**. Ideally, self attention will note the fact that \"la\" and \"maison\" are strongly coupled and, ideally, that the relative positions of \"maison\" and \"blanche\" are also strongly coupled as are the relative positions of \"white\" and \"house\". In the decoding stage of the model, in addition to the self attention over the target sequences another attention mechanism should pay attention to the fact that \"the\" and \"la\", \"house\" and \"maison\", and \"white\" and \"blanche\" are strongly coupled. At a minimum, therefore, we expect a successful seq2seq model to model self attention in both the encoding and decoding phases and source to target attention in the decoding phase. The optimal way to implement this is not known, but the transformer model implements an attention mechanism, described next, which empirically appears to be highly effective.\n",
        "\n",
        "\n",
        "### Prediction\n",
        "As noted above the a transformer is  used *autoregressively*: given a source, i.e., input, sequence $\\boldsymbol{x} = x_0, x_1,\\cdots, x_k, x_{k+1}$ of length $k+2$ tokens, where $x_0 \\equiv \\text{<sos>}$ denotes the **start of sequence** token and $x_{k+1} \\equiv \\text{<eos>}$ denotes the **end of sequence** token and the current output sequence  $\\boldsymbol{y}_{\\lt l} = y_0, y_1,\\cdots, y_{l-1}$ of length $l$ tokens, for every predicted target sub-sequence $\\boldsymbol{y}_{\\lt l}$ the model approximates a discrete conditional probability distribution  \n",
        "\\begin{align}\n",
        "p_{l} & \\equiv p(t_l \\in v_t| \\boldsymbol{x}, \\boldsymbol{y}_{\\lt l}),\n",
        "\\end{align}\n",
        " over the target vocabulary $v_t = \\text{<sos>}, \\text{<eos>}, v_1, \\cdots, v_{m}$, of size $m$ tokens, excluding the start and end tokens. For a given predicted target sub-sequence, its probability distribution is used to pick the next token $y_l$, which is appended to current predicted output sequence $y_{\\lt l}$ and the procedure is repeated until $y_l = \\text{<eos>}$ or the maximum allowed output sequence length is reached.\n",
        "\n",
        "For a vocabulary of size $m$ and a sequence of size $k$ (omitting the delimeters) every position in the sequence can be filled in $m$ ways. Therefore, there are $m^k$ possible sequences of which we want the most probable. Alas we have a bit of a computational problem. For example, for a sequence of size $k=85$ tokens and a target vocabulary of size $m = 28$ tokens there are $\\sim 10^{123}$ possible sentences. Even at a trillion probability calculations per second an exhaustive search would be an utterly futile undertaking because it would take far longer to complete than the current age of the universe ($\\sim 4 \\times 10^{17}$ s)! Obviously, we have no choice but to use a **heuristic strategy**.\n",
        "\n",
        "The simplest such strategy is the **greedy search** in which we choose the most probable token as the next token at position $l$.\n",
        "A potentially better strategy is **beam search** in which at each prediction stage we keep track of the $n$ most probable sequences so far. At the end we pick the most probable sequence among the $n$.\n",
        "\n",
        "\n",
        "### References\n",
        "  1.  [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "  1. [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "\n",
        "## Installation of `mlinphysics`\n",
        "\n",
        "### Local installation\n",
        "  ```bash\n",
        "      git clone https://github.com/hbprosper/mlinphysics\n",
        "      cd mlinphysics\n",
        "      pip install -e .\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOqAWa8EDOwD"
      },
      "source": [
        "## Running on Google Colab\n",
        "If on Google Colab (https://colab.research.google.com), execute cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krHMFgS5sP1e",
        "outputId": "3e0b41b7-cab0-439c-ca8a-3d3a7728fbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "\t1. uninstall mlinphysics\n",
            "\t2. sparse clone mlinphysics\n",
            "\n",
            "Cloning into 'mlinphysics'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "Receiving objects: 100% (46/46), 5.41 KiB | 5.41 MiB/s, done.\n",
            "remote: Total 46 (delta 0), reused 38 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "Receiving objects: 100% (5/5), 4.16 KiB | 4.16 MiB/s, done.\n",
            "remote: Total 5 (delta 0), reused 1 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "/content/mlinphysics\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (9/9), 27.17 KiB | 3.88 MiB/s, done.\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "\t3. install mlinphysics\n",
            "\n",
            "Obtaining file:///content/mlinphysics\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mlinphysics\n",
            "  Building editable for mlinphysics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mlinphysics: filename=mlinphysics-0.1.0-0.editable-py3-none-any.whl size=6113 sha256=2a8d49fb1e7c9afc8cb5f8c57a268c60444b93d84539dbd81a3fad0bb6b4046a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r1b9eisx/wheels/25/3d/7e/7f949d6009703f5d619afe25c502e793fc927a9f694d134564\n",
            "Successfully built mlinphysics\n",
            "Installing collected packages: mlinphysics\n",
            "Successfully installed mlinphysics-0.1.0\n",
            "/content\n",
            "\tRunning Google Colab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    url = f\"https://raw.githubusercontent.com/hbprosper/mlinphysics/refs/heads/main\"\n",
        "    !wget -q {url}/clone2colab.ipynb -O clone2colab.ipynb\n",
        "    %run clone2colab.ipynb\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zAi4I2osNkVw"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import importlib\n",
        "import shutil\n",
        "import random\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ML in physics module\n",
        "import mlinphysics.nn as mlp\n",
        "import mlinphysics.utils.data as dat\n",
        "import mlinphysics.utils.monitor as mon\n",
        "import mlinphysics.utils.tutorials as tut\n",
        "import mlinphysics.utils.transformer as tnm\n",
        "\n",
        "# update fonts\n",
        "\n",
        "plt.rcParams.update({\n",
        "  \"text.usetex\": shutil.which('latex') is not None,\n",
        "  \"font.family\": \"sans-serif\",\n",
        "  \"font.sans-serif\": \"Helvetica\",\n",
        "  \"font.size\": 14\n",
        "  })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89tXjN5bDOwE"
      },
      "source": [
        "## Computational device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Iml0HTm6DOwF",
        "outputId": "2aadcaa4-6e13-453c-b763-a8584946a2cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computational device: cpu\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'\\nComputational device: {str(DEVICE):s}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vy7iBa6DNkVy"
      },
      "outputs": [],
      "source": [
        "# SEED = 42\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# torch.cuda.manual_seed(SEED)\n",
        "# torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6nWA2wTDOwF"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Js-3iXU5DOwG"
      },
      "outputs": [],
      "source": [
        "short_tutorial = True\n",
        "\n",
        "if short_tutorial:\n",
        "\n",
        "    if IN_COLAB:\n",
        "        DATAFILE = 'seq2seq_series_2terms.txt'\n",
        "        dat.download(DATAFILE)\n",
        "    else:\n",
        "        DATAFILE = '../data/seq2seq_series_2terms.txt'\n",
        "\n",
        "    MAX_SEQ_LEN= 85\n",
        "\n",
        "    # model hyperparameters\n",
        "    ENC_EMB_DIM= 64    # dimension of embedding vector space\n",
        "    ENC_LAYERS = 2     # number of encoder layers\n",
        "    ENC_HEADS  = 8     # number of attention heads\n",
        "    ENC_FF_DIM = 128   # \"hidden\" dimension of feed-forward network\n",
        "    ENC_DROPOUT= 0.1\n",
        "\n",
        "    DEC_EMB_DIM= 64    # dimension of embedding vector space\n",
        "    DEC_LAYERS = 2     # number of decoder layers\n",
        "    DEC_HEADS  = 8     # number od decoder heads\n",
        "    DEC_FF_DIM = 128\n",
        "    DEC_DROPOUT= 0.1\n",
        "\n",
        "    # training hyperparameters\n",
        "    BATCH_SIZE    = 32\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NITERATIONS   = 400_000\n",
        "    STEP          =    100\n",
        "\n",
        "else:\n",
        "    DATAFILE = '../data/seq2seq_series.txt'\n",
        "    MAX_SEQ_LEN= 200\n",
        "\n",
        "    # model hyperparameters\n",
        "    ENC_EMB_DIM= 256   # dimension of embedding vector space\n",
        "    ENC_LAYERS = 4     # number of encoder layers\n",
        "    ENC_HEADS  = 8\n",
        "    ENC_FF_DIM = 1024  # \"hidden\" dimension of feed-forward network\n",
        "    ENC_DROPOUT= 0.1\n",
        "\n",
        "    DEC_EMB_DIM= 256   # dimension of embedding vector space\n",
        "    DEC_LAYERS = 4\n",
        "    DEC_HEADS  = 8\n",
        "    DEC_FF_DIM = 1024\n",
        "    DEC_DROPOUT= 0.1\n",
        "\n",
        "    BATCH_SIZE    = 64\n",
        "    DROPOUT       = 0.1\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NITERATIONS   = 400_000\n",
        "    STEP          =    100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpE9QgwvNkVy"
      },
      "source": [
        "## Read Sequence Data\n",
        "\n",
        "The file **seq2seq_series_2terms.txt** contains (source, target) pairs where the targets are the Taylor series expansions of the corresponding sources up to an error term of ${\\cal O}(x^6)$ and the sources are functions built from one or two terms randomly sampled from the set `{exp, sin, cos, tan, sinh, cosh, tanh}`. Since the source sequences are reasonably simple functions it is possible to train a transformer model to predict their Taylor series expansions in under an hour on a GPU. The more complicated functions in the file **seq2seq_series.txt** require more time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ruQyx8vUDOwG",
        "outputId": "2b6a2357-6642-48e8-8f6f-e6a59bb89fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\treading sequences\n",
            "\n",
            "\tsample size: 14367\n",
            "\n",
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "cosh(a*x)**3 + tanh(b*x)"
            ],
            "text/latex": "$\\displaystyle \\cosh^{3}{\\left(a x \\right)} + \\tanh{\\left(b x \\right)}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1 + b*x - b**3*x**3/3 + 2*b**5*x**5/15 + 3*a**2*x**2/2 + 7*a**4*x**4/8 + O(x**6)"
            ],
            "text/latex": "$\\displaystyle 1 + b x - \\frac{b^{3} x^{3}}{3} + \\frac{2 b^{5} x^{5}}{15} + \\frac{3 a^{2} x^{2}}{2} + \\frac{7 a^{4} x^{4}}{8} + O\\left(x^{6}\\right)$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "exp(a*x)*cosh(h*x)**2"
            ],
            "text/latex": "$\\displaystyle e^{a x} \\cosh^{2}{\\left(h x \\right)}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1 + x**2*(a**2/2 + h**2) + x**3*(a**3/6 + a*h**2) + x**4*(a**4/24 + a**2*h**2/2 + h**4/3) + x**5*(a**5/120 + a**3*h**2/6 + a*h**4/3) + a*x + O(x**6)"
            ],
            "text/latex": "$\\displaystyle 1 + x^{2} \\left(\\frac{a^{2}}{2} + h^{2}\\right) + x^{3} \\left(\\frac{a^{3}}{6} + a h^{2}\\right) + x^{4} \\left(\\frac{a^{4}}{24} + \\frac{a^{2} h^{2}}{2} + \\frac{h^{4}}{3}\\right) + x^{5} \\left(\\frac{a^{5}}{120} + \\frac{a^{3} h^{2}}{6} + \\frac{a h^{4}}{3}\\right) + a x + O\\left(x^{6}\\right)$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tan(d*x)/tanh(c*x)"
            ],
            "text/latex": "$\\displaystyle \\frac{\\tan{\\left(d x \\right)}}{\\tanh{\\left(c x \\right)}}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "d/c + x**2*(c*d/3 + d**3/(3*c)) + x**4*(-c**3*d/45 + c*d**3/9 + 2*d**5/(15*c)) + O(x**6)"
            ],
            "text/latex": "$\\displaystyle \\frac{d}{c} + x^{2} \\left(\\frac{c d}{3} + \\frac{d^{3}}{3 c}\\right) + x^{4} \\left(- \\frac{c^{3} d}{45} + \\frac{c d^{3}}{9} + \\frac{2 d^{5}}{15 c}\\right) + O\\left(x^{6}\\right)$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "13500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sinh(c*x) - cosh(m*x)"
            ],
            "text/latex": "$\\displaystyle \\sinh{\\left(c x \\right)} - \\cosh{\\left(m x \\right)}$"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "-1 - m**2*x**2/2 - m**4*x**4/24 + c*x + c**3*x**3/6 + c**5*x**5/120 + O(x**6)"
            ],
            "text/latex": "$\\displaystyle -1 - \\frac{m^{2} x^{2}}{2} - \\frac{m^{4} x^{4}}{24} + c x + \\frac{c^{3} x^{3}}{6} + \\frac{c^{5} x^{5}}{120} + O\\left(x^{6}\\right)$"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\tbuilding source vocabulary\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'a': 20, 'b': 21, 'c': 22, 'cos': 23, 'cosh': 24, 'd': 25, 'exp': 26, 'f': 27, 'g': 28, 'h': 29, 'm': 30, 'n': 31, 'sin': 32, 'sinh': 33, 'tan': 34, 'tanh': 35, 'x': 36}\n",
            "\n",
            "\tbuilding target vocabulary\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'O(x**6)': 20, 'a': 21, 'b': 22, 'c': 23, 'd': 24, 'f': 25, 'g': 26, 'h': 27, 'm': 28, 'x': 29}\n",
            "\n",
            "\ttokenize sources\n",
            " 14000\n",
            "\ttokenize targets\n",
            " 14000\n",
            "\tpad sequences and bracket with <sos> and <eos>\n",
            "\n",
            "Summary\n",
            " sample size: 11850\n",
            "   source sequence length:       22\n",
            "   source vocabulary size:       37\n",
            "\n",
            "   target sequence length:       85\n",
            "   target vocabulary size:       30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(tnm)\n",
        "\n",
        "seqdata = tnm.SequenceData(DATAFILE, max_seq_len=MAX_SEQ_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWc7RecHDOwH",
        "outputId": "4b056f2c-2cf0-4dee-a241-5cd167824b75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 1, 24,  3, 20,  5, 36,  4,  6, 13,  7, 35,  3, 21,  5, 36,  4,  0,\n",
              "         0,  0,  0,  0,  2]),\n",
              " array([ 1, 11,  7, 22,  5, 29,  7, 13,  5, 21,  6, 12,  5, 29,  6, 12,  9,\n",
              "        12,  8, 22,  6, 13,  5, 29,  6, 13,  9, 13,  7, 17,  5, 21,  6, 14,\n",
              "         5, 29,  6, 14,  9, 18,  7, 12,  5, 22,  6, 15,  5, 29,  6, 15,  9,\n",
              "        11, 15,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seqdata.sources[0], seqdata.targets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_PZ8kbDOwH"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POMUZbrLDOwH",
        "outputId": "bd86200f-28a9-4d08-a829-2384a65b59dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ndata = len(seqdata.sources)\n",
        "train_size = 11_000\n",
        "test_size  =    750\n",
        "val_size = ndata - train_size - test_size\n",
        "val_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxkKiNSPDOwH",
        "outputId": "6b6c2fb5-75bf-4d0d-ecbe-2a72a8fc1512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\tconfiguration\n",
            "\n",
            "name: TNN\n",
            "file:\n",
            "  losses: runs/test/TNN_losses.csv\n",
            "  params: runs/test/TNN_params.pth\n",
            "  init_params: runs/test/TNN_init_params.pth\n",
            "  plots: runs/test/TNN_plots.png\n",
            "train_size: 11000\n",
            "val_size: 100\n",
            "test_size: 750\n",
            "batch_size: 32\n",
            "monitor_step: 100\n",
            "delete: true\n",
            "frac: 0.01\n",
            "n_steps: 1\n",
            "n_iterations: 400000\n",
            "n_iters_per_step: 400000\n",
            "base_lr: 0.0002\n",
            "gamma: 0.5\n",
            "DATAFILE: ../data/seq2seq_series_2terms.txt\n",
            "MAX_SEQ_LEN: 85\n",
            "ENC_EMB_DIM: 64\n",
            "ENC_LAYERS: 2\n",
            "ENC_HEADS: 8\n",
            "ENC_FF_DIM: 128\n",
            "ENC_DROPOUT: 0.1\n",
            "DEC_EMB_DIM: 64\n",
            "DEC_LAYERS: 2\n",
            "DEC_HEADS: 8\n",
            "DEC_FF_DIM: 128\n",
            "DEC_DROPOUT: 0.1\n",
            "SRC_SEQ_LEN: 22\n",
            "SRC_VOCAB_SIZE: 37\n",
            "TRG_SEQ_LEN: 85\n",
            "TRG_VOCAB_SIZE: 30\n",
            "PAD_CODE: 0\n",
            "SOS_CODE: 1\n",
            "EOS_CODE: 2\n",
            "\n",
            "\n",
            "Save configuration to file runs/test/TNN_config.yaml\n",
            "\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(mlp)\n",
        "\n",
        "# name of model\n",
        "# -----------------------------------------\n",
        "name = 'TNN'\n",
        "\n",
        "# choose whether to create or load a configuration file\n",
        "load_existing_config = False\n",
        "\n",
        "if load_existing_config:\n",
        "    config = mlp.Config(f'{name}.yaml')\n",
        "else:\n",
        "    # create new configuration\n",
        "    config = mlp.Config(name, dirname='test')\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # training configuration\n",
        "    # ----------------------------------------\n",
        "    config('train_size',  train_size)   # training dataset size\n",
        "    config('val_size',    val_size)\n",
        "    config('test_size',   test_size)\n",
        "    config('batch_size',  BATCH_SIZE)   # number of graphs / batch\n",
        "    config('monitor_step',STEP)         # monitor training every n (=10) iterations\n",
        "    config('delete', True)              # if True recreate losses file before training\n",
        "    config('frac', 0.01)                # save model if average loss decreases by...\n",
        "                                        # ...more than a fraction \"frac\"\n",
        "    # ----------------------------------------\n",
        "    # optimizer / scheduler configuration\n",
        "    # ----------------------------------------\n",
        "    # a step comprises a given number of iterations\n",
        "    config('n_steps', 1)                # number of training steps\n",
        "    config('n_iterations', NITERATIONS)\n",
        "    config('n_iters_per_step', int(config('n_iterations') / config('n_steps')))\n",
        "    config('base_lr', LEARNING_RATE)    # initial learning rate\n",
        "    config('gamma', 0.5)                # learning rate scale factor\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # data\n",
        "    # ----------------------------------------\n",
        "    config('DATAFILE',    DATAFILE)\n",
        "    config('MAX_SEQ_LEN', MAX_SEQ_LEN)\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # model specification\n",
        "    # ----------------------------------------\n",
        "    # ENCODER\n",
        "    config('ENC_EMB_DIM', ENC_EMB_DIM)  # dimension of embedding vector space\n",
        "    config('ENC_LAYERS',  ENC_LAYERS)   # number of encoder layers\n",
        "    config('ENC_HEADS',   ENC_HEADS)    # number of attention heads\n",
        "    config('ENC_FF_DIM',  ENC_FF_DIM)   # \"hidden\" dimension of feed-forward network\n",
        "    config('ENC_DROPOUT', ENC_DROPOUT)\n",
        "\n",
        "    # DECODER\n",
        "    config('DEC_EMB_DIM', DEC_EMB_DIM)  # dimension of embedding vector space\n",
        "    config('DEC_LAYERS',  DEC_LAYERS)   # number of encoder layers\n",
        "    config('DEC_HEADS',   DEC_HEADS)    # number of attention heads\n",
        "    config('DEC_FF_DIM',  DEC_FF_DIM)   # \"hidden\" dimension of feed-forward network\n",
        "    config('DEC_DROPOUT', DEC_DROPOUT)\n",
        "\n",
        "\n",
        "config('SRC_SEQ_LEN',      seqdata.SRC_SEQ_LEN)\n",
        "config('SRC_VOCAB_SIZE',   seqdata.SRC_VOCAB_SIZE)\n",
        "\n",
        "config('TRG_SEQ_LEN',      seqdata.TRG_SEQ_LEN)\n",
        "config('TRG_VOCAB_SIZE',   seqdata.TRG_VOCAB_SIZE)\n",
        "\n",
        "config('PAD_CODE',    seqdata.PAD)\n",
        "config('SOS_CODE',    seqdata.SOS)\n",
        "config('EOS_CODE',    seqdata.EOS)\n",
        "\n",
        "print('\\n\\tconfiguration\\n')\n",
        "print(config)\n",
        "\n",
        "# learning rate scale factor\n",
        "\n",
        "print(f'\\nSave configuration to file {config.cfg_filename}\\n')\n",
        "\n",
        "config.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u3LfoI4DOwH"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FjMvZy4DOwH",
        "outputId": "4bfc7b11-4b81-46f6-810d-e72fbb4e0eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training data\n",
            "Dataset\n",
            "  shape of x: torch.Size([11000, 22])\n",
            "  shape of y: torch.Size([11000, 85])\n",
            "\n",
            "training data for validation\n",
            "Dataset\n",
            "  shape of x: torch.Size([100, 22])\n",
            "  shape of y: torch.Size([100, 85])\n",
            "\n",
            "validation data\n",
            "Dataset\n",
            "  shape of x: torch.Size([100, 22])\n",
            "  shape of y: torch.Size([100, 85])\n",
            "\n",
            "test data\n",
            "Dataset\n",
            "  shape of x: torch.Size([750, 22])\n",
            "  shape of y: torch.Size([750, 85])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(dat)\n",
        "\n",
        "train_size = config('train_size')\n",
        "val_size   = config('val_size')\n",
        "test_size  = config('test_size')\n",
        "\n",
        "# training dataset (this defines the empirical risk to be minimized)\n",
        "print('training data')\n",
        "train_data = dat.Dataset(seqdata.sources,\n",
        "                         start=0,\n",
        "                         end=train_size,\n",
        "                         targets=seqdata.targets)\n",
        "\n",
        "# a random subset of the training data to check for overtraining\n",
        "# by comparing with the empirical risk from the validation set\n",
        "print('training data for validation')\n",
        "train_data_val = dat.Dataset(seqdata.sources,\n",
        "                             start=0,\n",
        "                             end=train_size,\n",
        "                             targets=seqdata.targets,\n",
        "                             random_sample_size=val_size)\n",
        "\n",
        "# validation dataset (for monitoring training)\n",
        "print('validation data')\n",
        "val_data = dat.Dataset(seqdata.sources,\n",
        "                       start=train_size,\n",
        "                       end=train_size + val_size,\n",
        "                       targets=seqdata.targets)\n",
        "\n",
        "# test dataset\n",
        "print('test data')\n",
        "test_data= dat.Dataset(seqdata.sources,\n",
        "                       start=train_size + val_size,\n",
        "                       end=train_size + val_size + test_size,\n",
        "                       targets=seqdata.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9f8C4NHDOwH",
        "outputId": "97781ebf-531e-49cc-9f44-67cf631e5307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TRAIN_DATA\n",
            " source: tensor([ 1, 24,  3, 20,  5, 36,  4,  6, 13,  7, 35,  3, 21,  5, 36,  4,  0,  0,\n",
            "         0,  0,  0,  2])\n",
            " target: tensor([ 1, 11,  7, 22,  5, 29,  7, 13,  5, 21,  6, 12,  5, 29,  6, 12,  9, 12,\n",
            "         8, 22,  6, 13,  5, 29,  6, 13,  9, 13,  7, 17,  5, 21,  6, 14,  5, 29,\n",
            "         6, 14,  9, 18,  7, 12,  5, 22,  6, 15,  5, 29,  6, 15,  9, 11, 15,  7,\n",
            "        20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2])\n",
            "\n",
            "TRAIN_DATA_VAL\n",
            " source: tensor([ 1, 32,  3, 21,  5, 36,  4,  5, 23,  3, 20,  5, 36,  4,  0,  0,  0,  0,\n",
            "         0,  0,  0,  2])\n",
            " target: tensor([ 1, 22,  5, 29,  7, 29,  6, 13,  5,  3,  8, 21,  6, 12,  5, 22,  9, 12,\n",
            "         8, 22,  6, 13,  9, 16,  4,  7, 29,  6, 15,  5,  3, 21,  6, 14,  5, 22,\n",
            "         9, 12, 14,  7, 21,  6, 12,  5, 22,  6, 13,  9, 11, 12,  7, 22,  6, 15,\n",
            "         9, 11, 12, 10,  4,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2])\n",
            "\n",
            "VAL_DATA\n",
            " source: tensor([ 1,  8, 34,  3, 28,  5, 36,  4,  7, 35,  3, 29,  5, 36,  4,  6, 13,  0,\n",
            "         0,  0,  0,  2])\n",
            " target: tensor([ 1,  8, 26,  5, 29,  7, 29,  6, 13,  5,  3,  8, 26,  6, 13,  9, 13,  7,\n",
            "        27,  6, 13,  4,  7, 29,  6, 15,  5,  3,  8, 12,  5, 26,  6, 15,  9, 11,\n",
            "        15,  8, 27,  6, 15,  4,  7, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2])\n",
            "\n",
            "TEST_DATA\n",
            " source: tensor([ 1, 33,  3, 21,  5, 36,  4,  6, 13,  5, 24,  3, 25,  5, 36,  4,  6, 12,\n",
            "         0,  0,  0,  2])\n",
            " target: tensor([ 1, 22,  6, 13,  5, 29,  6, 13,  7, 29,  6, 15,  5,  3, 22,  6, 15,  9,\n",
            "        12,  7, 22,  6, 13,  5, 24,  6, 12,  4,  7, 20,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def printme(text, data, ii):\n",
        "    print(text)\n",
        "    print(' source:', data[ii][0])\n",
        "    print(' target:', data[ii][1])\n",
        "    print()\n",
        "\n",
        "ii = 0\n",
        "printme('TRAIN_DATA', train_data, ii)\n",
        "printme('TRAIN_DATA_VAL', train_data_val, ii)\n",
        "printme('VAL_DATA', val_data, ii)\n",
        "printme('TEST_DATA', test_data, ii)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qCE09odDOwI"
      },
      "source": [
        "## DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJI6CVpHDOwI",
        "outputId": "82a0ccd7-48f9-4737-cac0-3fa5d7dcf561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train data loader\n",
            "DataLoader\n",
            "  Number of iterations has been specified\n",
            "  maxiter:          400000\n",
            "  batch_size:           32\n",
            "  shuffle_step:        343\n",
            "\n",
            "train data loader for validation\n",
            "DataLoader\n",
            "  maxiter:               1\n",
            "  batch_size:          100\n",
            "  shuffle_step:          1\n",
            "\n",
            "validation data loader\n",
            "DataLoader\n",
            "  maxiter:               1\n",
            "  batch_size:          100\n",
            "  shuffle_step:          1\n",
            "\n",
            "test data loader\n",
            "DataLoader\n",
            "  maxiter:             750\n",
            "  batch_size:            1\n",
            "  shuffle_step:        750\n",
            "\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(dat)\n",
        "\n",
        "print('train data loader')\n",
        "train_loader = dat.DataLoader(train_data,\n",
        "                              batch_size=config('batch_size'),\n",
        "                              num_iterations=config('n_iterations'))\n",
        "\n",
        "print('train data loader for validation')\n",
        "train_loader_val = dat.DataLoader(train_data_val,\n",
        "                                  batch_size=len(train_data_val))\n",
        "\n",
        "print('validation data loader')\n",
        "val_loader = dat.DataLoader(val_data,\n",
        "                            batch_size=len(val_data))\n",
        "\n",
        "print('test data loader')\n",
        "test_loader = dat.DataLoader(test_data,\n",
        "                             batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rABspL0wNkVz"
      },
      "source": [
        "## The Model\n",
        "\n",
        "The transformer comprises an **encoder** and **decoder**, each of which consists of one or more processing layers.\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder does the following:\n",
        " 1. Each token in the source (input) sequence is encoded as a vector $\\boldsymbol{t}$ in a space of $d =$ **emb_dim** dimensions. A sequence is therefore represented as a point cloud in the vector space.\n",
        " 1. The position of each token is also encoded as a vector $\\boldsymbol{p}$ in a vector space of the same dimension as $\\boldsymbol{t}$. We can think of these vectors $\\boldsymbol{p}$ as residing in the same vector space as the vectors $\\boldsymbol{t}$.  Both the token and position embeddings are trainable.\n",
        " 1. Each token is associated with a third vector: $\\boldsymbol{v} = \\lambda \\boldsymbol{t} + \\boldsymbol{p}$, where the scale factor $\\lambda = \\sqrt{d}$.  In this tutorial, we make $\\lambda$ a tunable parameter.\n",
        "\n",
        "The vectors $\\boldsymbol{v}$ are processed through $N$ *encoder layers*.\n",
        "\n",
        "Since the source sequences are **padded** so that they are all of equal length, a method is needed to ensure that the pad tokens are ignored in all calculations. This is done using **masks**.\n",
        "The source mask, `src_mask`, has value 0 if the token in the source is a `<pad>` token and 1 otherwise. There is also a target mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3iANFmUNkVz"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size,      # vocabulary size (of source)\n",
        "                 max_len,         # maximum number of tokens per sequence\n",
        "                 emb_dim,         # dimension of token embedding space\n",
        "                 n_layers,        # number of encoding layers\n",
        "                 n_heads,         # number of attention heads per encoding layer\n",
        "                 ff_dim,          # dimension of feed-forward network\n",
        "                 dropout,         # dropout probability\n",
        "                 device):         # computational device\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        print(f'''\n",
        "    Encoder\n",
        "    -------\n",
        "      vocabulary size:       {vocab_size:10d}\n",
        "      sequence length:       {max_len:10d}\n",
        "      embedding dimension:   {emb_dim:10d}\n",
        "      number of layers:      {n_layers:10d}\n",
        "      number of heads:       {n_heads:10d}\n",
        "      hidden dim. of FFN:    {ff_dim:10d}\n",
        "        ''')\n",
        "\n",
        "        # cache computational device\n",
        "        self.device = device\n",
        "\n",
        "        # represent each of the 'vocab_size' tokens by a vector\n",
        "        # of size d = emb_dim. nn.Embedding \"learns\" a simple\n",
        "        # lookup table that maps the code for each token in the\n",
        "        # vocabulary to a vector of size emb_dim.\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # represent the position of each token by a vector of\n",
        "        # size d = emb_dim.\n",
        "        # 'max_len' is the maximum length of a sequence.\n",
        "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
        "\n",
        "        # create 'n_layers' encoding layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(emb_dim,\n",
        "                                                  n_heads,\n",
        "                                                  ff_dim,\n",
        "                                                  dropout,\n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        # randomly set to zero weights during training.\n",
        "        # dropout is thought to mitigate over-training\n",
        "        self.dropout= nn.Dropout(dropout)\n",
        "\n",
        "        # factor by which to scale token embedding vectors.\n",
        "        # use nn.Parameter to tell PyTorch that this is a\n",
        "        # tunable parameter.\n",
        "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # src      : [batch_size, src_len]         (shape of src)\n",
        "        # src_mask : [batch_size, 1, 1, src_len]   (shape of src_mask)\n",
        "\n",
        "        batch_size, src_len = src.shape\n",
        "\n",
        "        # ---------------------------------------\n",
        "        # token embedding\n",
        "        # ---------------------------------------\n",
        "        src = self.tok_embedding(src)\n",
        "        # src: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # ---------------------------------------\n",
        "        # token position embedding\n",
        "        # ---------------------------------------\n",
        "        # create a row tensor, p, with entries [0, 1,..., src_len-1]\n",
        "        pos = torch.arange(0, src_len)\n",
        "        # pos: [src_len]\n",
        "\n",
        "        # 1. add a dimension at position 0 (for batch size)\n",
        "        # 2. repeat one instance of p per row 'batch_size'\n",
        "        #    times so that we obtain\n",
        "        # pos = |p|\n",
        "        #       |p|\n",
        "        #        :\n",
        "        #       |p|\n",
        "        # 3. send to computational device\n",
        "        once_per_row = 1\n",
        "        #   3.1 unsqueeze inserts a dimension, here dimension 0\n",
        "        #       so that pos has shape [1, src_len].\n",
        "        #   3.2 repeat this row of integers batch_size times,\n",
        "        #       once per row\n",
        "        #   3.3 send to computational device\n",
        "        pos = pos.unsqueeze(0).repeat(batch_size, once_per_row).to(self.device)\n",
        "        # pos: [batch_size, src_len]\n",
        "\n",
        "        # the embedding maps every token ordinal value (position) to a vector\n",
        "        # in a vector the embedding space.\n",
        "        pos = self.pos_embedding(pos)\n",
        "        # pos: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # linearly combine token and token position embeddings.\n",
        "        # (perhaps this could be replaced by an MLP?)\n",
        "        src = src * self.scale + pos\n",
        "        # src: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # it is not clear how much this helps, but lets keep it.\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        # now pass embedded vectors through encoding layers.\n",
        "        # Note: every token in the sequence src is processed\n",
        "        # simultaneously.\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            # src: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # return the vectors representing the processed tokens.\n",
        "        # the tensor src will be fed into the decoder along with\n",
        "        # the target tensor.\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE4K943iNkV0"
      },
      "source": [
        "### Encoder Layer\n",
        "\n",
        " 1. Pass the source tensor and its mask to the **multi-head attention** layers.\n",
        " 1. Apply a residual connection and [Layer Normalization](https://arxiv.org/abs/1607.06450).\n",
        " 1. Apply a linear layer.\n",
        " 1. Apply a residual connection and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGmYBW5PNkV1"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 emb_dim, # token embedding dimension\n",
        "                 n_heads, # number of attention \"heads\"\n",
        "                 ff_dim,  # dimension of feed-forward network\n",
        "                 dropout, # dropout probability\n",
        "                 device): # computational device\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention       = MultiHeadAttention(emb_dim,\n",
        "                                                       n_heads,\n",
        "                                                       dropout,\n",
        "                                                       device)\n",
        "\n",
        "        self.self_attention_norm  = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.feedforward          = Feedforward(emb_dim, ff_dim, dropout)\n",
        "\n",
        "        self.feedforward_norm     = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.dropout              = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # src      : [batch_size, src_len, emb_dim]\n",
        "        # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "        # ------------------------------------------\n",
        "        # self attention over embedded source tensor\n",
        "        # ------------------------------------------\n",
        "        # distinguish between src and src_ as the\n",
        "        # former is needed later for a residual connection.\n",
        "        # the output of the self_attention layer are vectors\n",
        "        # that incorporate semantic and syntactic information\n",
        "        # about the input tokens.\n",
        "        #\n",
        "        # Note, for self attention:\n",
        "        #   Q = src\n",
        "        #   K = src\n",
        "        #   V = src\n",
        "        src_ = self.self_attention(src, src, src, src_mask)\n",
        "        # src_: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # how useful is this?\n",
        "        src_ = self.dropout(src_)\n",
        "\n",
        "        # ------------------------------------------\n",
        "        # add a residual connection, followed by\n",
        "        # layer normalization.\n",
        "        # ------------------------------------------\n",
        "        # distinguish between src and src+src_ as the\n",
        "        # former is later needed for another\n",
        "        # residual connection.\n",
        "        src  = self.self_attention_norm(src + src_)\n",
        "        # src: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        # apply a feed-forward network\n",
        "        src_ = self.feedforward(src)\n",
        "        # src_: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        src_ = self.dropout(src_)\n",
        "\n",
        "        # add residual connection and layer normalization\n",
        "        src  = self.feedforward_norm(src + src_)\n",
        "        # src: [batch_size, src_len, emb_dim]\n",
        "\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTKA5UTANkV1"
      },
      "source": [
        "## Multi-Head Attention Layer\n",
        "\n",
        "Attention is the key to the transformer model.  Attention, in this model, is defined by the matrix expression\n",
        "\n",
        "\\begin{align}\n",
        "    \\texttt{Attention}(Q, K, V) & = \\texttt{Softmax}\\left(\\frac{Q K^T}{\\sqrt{d}} \\right) V,\n",
        "\\end{align}\n",
        "\n",
        "where $Q$ is called the `query`, $K$ the `key`, $V$ the `value`, and $d =$ **emb_dim** is the dimension of the vectors that represent the tokens. The Google researchers found that it is better to split each vector representing a token into **n_heads** smaller vectors each of size\n",
        "$$\\textrm{\\bf head\\_dim} = d / \\textrm{\\bf n\\_heads}.$$\n",
        "The integer **n_heads** is the number of so-called **attention heads**. It is claimed, with some justification in the Google paper, that each head pays attention to different aspects of a sequence. It is certainly plausible that this splitting procedure enhances the flexibility of the model, however, at our current level of understanding of how functions with millions of parameters truly work, such claims should nonetheless be taken with a liberal pinch of salt.\n",
        "\n",
        "In self attention, the query, key, and value tensors are derived from the *same* tensor, either the source or target tensor, via separate linear transformations of that tensor (see *Attention Algorithm* below). The coefficients of the linear functions are free parameters to be determined by the training algorithm.  The number of rows in $Q$, $K$, and $V$, namely, **query_len**,  **key_len**, and **value_len**, respectively, is equal to the sequence length **seq_len**. For target/source attention, the query is a linear function of the target tensor while the key and value tensors are linear functions of the source tensor, where, again, the coefficients are free parameters to be fitted during training.\n",
        "\n",
        "We first describe the attention mechanism mathematically and then follow with an algorithmic description that closely follows\n",
        "the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/). It is to be understood that every operation described below is performed for a batch of sequences. Therefore, when we refer to a matrix we really mean a batch of matrices.\n",
        "\n",
        "Because each vector has been split into n_heads vectors of dimension $head\\_dim$, the calculations below are applied separately to each such vector. At the end, the smaller vectors are coalesced back into vectors of the embedding dimension. First consider the matrix product $Q K^T$ in component form, where summation over repeated indices (the Einstein convention) is implied,\n",
        "\\begin{align}\n",
        "A_{qk}\n",
        "& = Q_{q h} \\, [K^T]_{hk}, \\nonumber\\\\\n",
        "& \\quad q=1,\\cdots, \\text{query\\_len}, \\,\\, h = 1, \\cdots, \\text{head\\_dim}, \\,\\, k = 1, \\cdots, \\text{key\\_len} .\n",
        "\\end{align}\n",
        "When the matrix $A$ is scaled and a softmax function applied elementwise along the key length dimension (here, horizontally) the result is another matrix $W$ whose row elements, by construction, sum to unity. The matrix $W$, which is a matrix of normalized weights, is then multiplied by $V$ to yield the matrix of sub-vectors\n",
        "\\begin{align}\n",
        "    \\text{Attention}_{qh}  \n",
        "    & = W_{qk} V_{kh},\n",
        "\\end{align}\n",
        "which encodes information about the degree of association between the smaller vectors, each associated with a token.\n",
        "\n",
        "Since tokens are represented by vectors, it is instructive to think of the attention computation geometrically.   Each row, $i$, of $Q$, $K$, and $V$ can be regarded as the vectors $\\boldsymbol{q}_i$, $\\boldsymbol{k}_i$, and $\\boldsymbol{v}_i$, respectively, associated with token $i$, where each vector (really sub-vector) is of dimension head_dim.  Consider, for example, a sequence with **seq_len** = 2. We can write $Q$, $K$, and $V$ as\n",
        "\n",
        "\\begin{align}\n",
        "Q & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right], \\\\\n",
        "K & = \\left[\\begin{matrix} \\boldsymbol{k}_1 \\\\ \\boldsymbol{k}_2 \\end{matrix}\\right], \\text{ and} \\\\\n",
        "V & = \\left[\\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\end{matrix}\\right] ,\n",
        "\\end{align}\n",
        "\n",
        "and $A = Q K^T$ as the outer product matrix\n",
        "\n",
        "\\begin{align}\n",
        "A & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right]\n",
        "\\left[\\begin{matrix} \\boldsymbol{k}_1 & \\boldsymbol{k}_2 \\end{matrix}\\right] ,\n",
        "\\nonumber\\\\\n",
        "& = \\left[\n",
        "\\begin{matrix}\n",
        "\\boldsymbol{q}_1\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_1\\cdot \\boldsymbol{k}_2 \\\\\n",
        "\\boldsymbol{q}_2\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_2\\cdot \\boldsymbol{k}_2\n",
        "\\end{matrix}\n",
        "\\right] .\n",
        "\\end{align}\n",
        "\n",
        "The matrix $A$ can be interpreted as a measure of the degree to which the $\\boldsymbol{q}$ and $\\boldsymbol{k}$ vectors are aligned. Presumably, the more aligned the two vectors the stronger the relationship between the  tokens they represent. Because of the use of the dot product, the degree of alignment depends both on the angle between the vectors as well as on their magnitudes. Consequently, two vectors can be more strongly aligned than a vector's alignment with itself!\n",
        "\n",
        "After the scaling and softmax operations on $A$, tokens 1 and 2 become associated with vectors $\\boldsymbol{w}_1 =  (w_{11}, w_{12})$ and $\\boldsymbol{w}_2 =  (w_{21}, w_{22})$, respectively, where\n",
        "\\begin{align}\n",
        "    w_{ij} & = \\frac{\\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_j \\, / \\, \\sqrt{d}\\right)}\n",
        "    {\\sum_{k = 1}^2 \\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_k \\, / \\, \\sqrt{d}\\right)} .\n",
        "\\end{align}\n",
        "\n",
        "These (weight) vectors lie in the line segment $[\\boldsymbol{p}_1, \\boldsymbol{p}_2]$ depicted in the figure below. The line segment is a simplex (here, a 1-simplex) that is embedded in a vector space of dimension **seq_len**.  In this vector space, tokens 1 and 2 are represented by the orthogonal unit vectors $\\boldsymbol{u}_1$ and $\\boldsymbol{u}_2$, respectively. For a sequence of length $n$, the vectors $\\boldsymbol{w}_i$, $i = 1,\\cdots, n$ lie in the $(n-1)$-simplex and, again, each coordinate unit vector $\\boldsymbol{u}_i$ represents a token.  \n",
        "<img src=\"https://github.com/hbprosper/mlinphysics/blob/main/Labs/10.Transformer/code/simplex.png?raw=1\" align=\"left\" width=\"250px\"/>\n",
        "The vector for token, $i$, is the weighted average\n",
        "\\begin{align}\n",
        "    \\text{Attention}_i & = w_{i1}  \\boldsymbol{v}_1 + w_{i2} \\boldsymbol{v}_2\n",
        "\\end{align}\n",
        "of the so-called value vectors $\\boldsymbol{v}_1$ and $\\boldsymbol{v}_2$. In both self attention and (source to target) attention, the value vectors are source token vectors.\n",
        "\n",
        "The upshot of this construction is that vectors representing the tokens are moved about in the embedding space in complex ways in accordance with the attention operation in such a way that their relative positions within that space encodes information about the degree and nature of the association between the tokens.\n",
        "<br clear=\"left\"/>\n",
        "\n",
        "\n",
        "### Attention Algorithm\n",
        "\n",
        "Now we describe the transformer attention mechanism algorithmically, following closely the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), but with some notational changes.\n",
        "\n",
        "<img src=\"https://github.com/hbprosper/mlinphysics/blob/main/Labs/10.Transformer/code/transformer.png?raw=1\" align=\"left\" width=\"500px\"/>\n",
        "\n",
        "#### Step 1\n",
        "As noted, the attention mechanism starts with three tensors, $V_\\text{in}$,  $K_\\text{in}$, and $Q_\\text{in}$ of shapes **[batch_size,query_len,emb_dim]**, **[batch_size,key_len,emb_dim]**, and **[batch_size,value_len,emb_dim]**, respectively, with **value_len = key_len**. (emb_dim is called hid_dim in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)).  For self attention, $V_\\text{in}$,  $K_\\text{in}$, and $Q_\\text{in}$ are the *same* tensor, while for target to source attention $Q_\\text{in}$ is associated with the target tensor and $K_\\text{in}$ and $V_\\text{in}$ with the source tensor.\n",
        "\n",
        "Three trainable linear layers, $f_V$, $f_K$, $f_Q$ are defined, each of shape **[emb_dim,emb_dim]**, which yield the so-called `query`, `key`, and `value` tensors\n",
        "\\begin{align}\n",
        "    V & = f_V(\\boldsymbol{V_\\text{in}}), \\\\\n",
        "    K & = f_K(\\boldsymbol{K_\\text{in}}), \\text{ and} \\\\\n",
        "    Q & = f_Q(\\boldsymbol{Q_\\text{in}}).\n",
        "\\end{align}\n",
        "Each tensor $Q$, $K$, and $V$ is the same shape as $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$, respectively.\n",
        "\n",
        "#### Step 2\n",
        "Tensors $Q$, $K$, and $V$ are reshaped by first splitting the embedding dimension, **emb_dim**, into **n_heads** blocks of size **head_dim = emb_dim / n_heads** so that their shapes become **[batch_size, seq_len, n_heads, head_dim]**, where the **seq_len** pertains to **query_len**, **key_len**, or **value_len**, whose value is determined at runtime. (This is why the source (and target) masks must have shape [batch_size, 1, 1, emb_dim] as will become clear below.)\n",
        "\n",
        "#### Step 3\n",
        "Dimensions 1 and 2 of the tensors $Q$, $K$, and $V$ are permuted (`Tensor.permute(0, 2, 1, 3)`) so that we now have **[batch_size, n_heads, seq_len, head_dim]**. Tensor $K$ is further permuted (`Tensor.permute(0, 1, 3, 2)`) to shape **[batch_size, n_heads, head_dim, seq_len]** so that it represents $K^T$.\n",
        "\n",
        "#### Step 4\n",
        "Tensor $A = Q K^T$ is computed using `torch.matmul(Q, K^T)`, scaled by $1 \\, / \\, \\sqrt{d}$, and a softmax is applied to the last dimension of $A$, that is, the sequence length dimension, yielding the tensor $W$ of shape **[batch_size, n_heads, query_len, key_len]**.\n",
        "\n",
        "#### Step 5\n",
        "$\\text{Attention} = W V$ is computed, yielding a tensor of shape\n",
        "**[batch_size, n_heads, query_len, head_dim]**.\n",
        "\n",
        "#### Step 6\n",
        "The n_heads and query_len dimensions of `Attention` are transposed (`Tensor.permute(0, 2, 1, 3)`) to shape **[batch_size, query_len, n_heads, head_dim]** and forced to be contiguous in memory (`contiguous()`).\n",
        "\n",
        "#### Step 7\n",
        "The **n_heads** vectors of dimension  **head_dim** are concatenated using `Attention.view(batch_size, seq_len, emb_dim)` to merge the attention heads into a single `MultiHeadAttention` tensor.\n",
        "\n",
        "#### Step 8\n",
        "Finally, the merged `MultiHeadAttention` tensor is pushed through a trainable linear layer of shape **[emb_dim, emb_dim]** to output a tensor of shape **[batch_size, seq_len, emb_dim]**.\n",
        "\n",
        "### Comments\n",
        "As noted above, it is claimed that the  algorithm above captures the notion of \"paying attention to\" token-token associations both within the same sequence and across sequences and that each attention head \"pays attention to\" a different aspect of the sequences. All such claims should be taken with a pinch of salt for at least two reasons.\n",
        "First, it is not at all obvious that this computation aligns with our intuitive understanding of  that notion and, second, the computation is nested through multiple attention layers. Therefore, whatever the attention layers are doing, it is distributed over multiple layers in a highly non-linear, non-local, way.\n",
        "\n",
        "It is, however, undeniable that the transformer has yielded amazing results. Therefore, we are forced to concede that, in practice,  whatever is going on in the attention layers the algorithm works wonders!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUD-6rRKNkV1"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, n_heads, dropout, device):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # emb_dim must be a multiple of n_heads\n",
        "        assert emb_dim % n_heads == 0\n",
        "\n",
        "        self.emb_dim  = emb_dim\n",
        "        self.n_heads  = n_heads\n",
        "        self.head_dim = emb_dim // n_heads\n",
        "\n",
        "        self.linear_Q = nn.Linear(emb_dim, emb_dim)\n",
        "        self.linear_K = nn.Linear(emb_dim, emb_dim)\n",
        "        self.linear_V = nn.Linear(emb_dim, emb_dim)\n",
        "        self.linear_O = nn.Linear(emb_dim, emb_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query  : [batch_size, query_len, emb_dim]\n",
        "        # key    : [batch_size, key_len,   emb_dim]\n",
        "        # value  : [batch_size, value_len, emb_dim]\n",
        "\n",
        "        batch_size, _, emb_dim = query.shape\n",
        "        assert emb_dim == self.emb_dim\n",
        "\n",
        "        Q = self.linear_Q(query)\n",
        "        # Q: [batch_size, query_len, emb_dim]\n",
        "\n",
        "        K = self.linear_K(key)\n",
        "        # K: [batch_size, key_len,   emb_dim]\n",
        "\n",
        "        V = self.linear_V(value)\n",
        "        # V: [batch_size, value_len, emb_dim]\n",
        "\n",
        "        # split vectors of size emb_dim into 'n_heads' vectors each\n",
        "        # of size 'head_dim' and then permute dimensions 1 and 2\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # K: [batch_size, n_heads, key_len,   head_dim]\n",
        "\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # transpose K (by permuting key_len and head_dim), then\n",
        "        # compute QK^T/scale\n",
        "        A = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        # A: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # apply (optional) map to ensure that pad tokens do\n",
        "        # not contribute to the attention calculation.\n",
        "        if mask is not None:\n",
        "            A = A.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # apply softmax to the last dimension (i.e, to key len)\n",
        "        # WARNING: W is referred to as 'attention' in Annotated Transformer!\n",
        "        W = torch.softmax(A, dim=-1)\n",
        "        # W: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # not sure why dropout is useful here\n",
        "        W = self.dropout(W)\n",
        "\n",
        "        # compute attention: (QK^T/scale)V\n",
        "        attention = torch.matmul(W, V)\n",
        "        # attention: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        # permute n_heads and query len and make sure the tensor\n",
        "        # is contiguous in memory...\n",
        "        attention = attention.permute(0, 2, 1, 3).contiguous()\n",
        "        # attention: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        # ... and concatenate the n heads into a single multi-head\n",
        "        # attention tensor.\n",
        "        # if attention is being applied to source sequences, then\n",
        "        # query_len = src_len. If applied to output sequences, then\n",
        "        # query_len = trg_len.\n",
        "        attention = attention.view(batch_size, -1, self.emb_dim)\n",
        "        # attention: [batch_size, query_len, emb_dim]\n",
        "\n",
        "        output = self.linear_O(attention)\n",
        "        # output: [batch_size, query_len, emb_dim]\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oacePmrSNkV2"
      },
      "source": [
        "### Feedforward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzlh65gaNkV2"
      },
      "outputs": [],
      "source": [
        "class Feedforward(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim, ff_dim, dropout):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear_1 = nn.Linear(emb_dim, ff_dim)\n",
        "\n",
        "        self.linear_2 = nn.Linear(ff_dim, emb_dim)\n",
        "\n",
        "        self.dropout  = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, emb_dim]\n",
        "\n",
        "        x = self.linear_1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        # x: [batch_size, seq_len, ff_dim]\n",
        "\n",
        "        x = self.linear_2(x)\n",
        "        # x: [batch_size, seq_len, emb_dim]\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlidARf5NkV2"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder takes the encoded representation of the source sequence, which is represented by a point cloud in a vector space of dimension **emb_dim**, together with the target sequence, or the current predicted output sequence, and computes weights over the target vocabulary which can be converted to a probability distribution over the target vocabulary for the next output token.\n",
        "\n",
        "The decoder has two multi-head attention layers: a *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value. The mask, as discussed below, is in addition to the one that masks out the pad tokens.\n",
        "\n",
        "**Note**: In PyTorch, the softmax operation, which converts the output weights to probabilities, is contained within the loss function, so the decoder does not have a softmax layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMQz3fxONkV3"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size,   # size of target vocabulary\n",
        "                 max_len,      # maximum output sequence length\n",
        "                 emb_dim,      # dimension of embedding vector space\n",
        "                 n_layers,     # number of decoder layers\n",
        "                 n_heads,      # number of masked attention heads\n",
        "                 ff_dim,       # hidden dimension of feed-forward network\n",
        "                 dropout,      # weight dropout probability\n",
        "                 device):      # computational device\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        print(f'''\n",
        "    Decoder\n",
        "    -------\n",
        "      vocabulary size:       {vocab_size:10d}\n",
        "      sequence length:       {max_len:10d}\n",
        "      embedding dimension:   {emb_dim:10d}\n",
        "      number of layers:      {n_layers:10d}\n",
        "      number of heads:       {n_heads:10d}\n",
        "      hidden dim. of FFN:    {ff_dim:10d}\n",
        "        ''')\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
        "\n",
        "        self.layers  = nn.ModuleList([DecoderLayer(emb_dim,\n",
        "                                                   n_heads,\n",
        "                                                   ff_dim,\n",
        "                                                   dropout,\n",
        "                                                   device)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.linear  = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = nn.Parameter(torch.sqrt(torch.FloatTensor([emb_dim])))\n",
        "\n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        # trg      : [batch_size, trg_len]\n",
        "        # src      : [batch_size, src_len, emb_dim]\n",
        "        # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "        # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "        batch_size, trg_len = trg.shape\n",
        "\n",
        "        # see Encoder for comments\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos: [batch_size, trg_len]\n",
        "\n",
        "        trg = self.tok_embedding(trg) * self.scale + self.pos_embedding(pos)\n",
        "        # trg: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        trg = self.dropout(trg)\n",
        "\n",
        "        # send the *same* source tensor to every decoding layer. however,\n",
        "        # analogously to the Encoder, in the Decoder the target tensor is\n",
        "        # processed through a sequence of layers.\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "            # trg: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        # for each output token, output 'vocab_size' weights,\n",
        "        # which later will be converted to probabilities.\n",
        "        output = self.linear(trg)\n",
        "        # output: [batch_size, trg_len, vocab_size]\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhZAhu4NkV3"
      },
      "source": [
        "### Decoder Layer\n",
        "\n",
        "The decoder layer has two multi-head attention layers, `self_attention` and `attention`. The former applies the attention algorithm to the target sequences, while the latter applies the algorithm between the target and source sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgYbJN_6NkV3"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 emb_dim,\n",
        "                 n_heads,\n",
        "                 ff_dim,\n",
        "                 dropout,\n",
        "                 device):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # attention within the target sequences\n",
        "        self.self_attention      = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
        "\n",
        "        self.self_attention_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        # attention between the source and target sequences\n",
        "        self.attention           = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
        "\n",
        "        self.attention_norm      = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.feedforward         = Feedforward(emb_dim, ff_dim, dropout)\n",
        "\n",
        "        self.feedforward_norm    = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        # trg      : [batch_size, trg_len, emb_dim]\n",
        "        # src      : [batch_size, src_len, emb_dim]\n",
        "        # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
        "        # src_mask : [batch_size, 1, 1, src_len]\n",
        "\n",
        "        # compute attention over embedded target sequences.\n",
        "        # distinguish between trg and trg_, since the former\n",
        "        # is needed later for residual connections.\n",
        "        #                          Q    K    V\n",
        "        trg_ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        # trg_: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        trg_ = self.dropout(trg_)\n",
        "\n",
        "        # residual connection and layer norm\n",
        "        trg  = self.self_attention_norm(trg + trg_)\n",
        "        # trg: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        # target to source attention\n",
        "        #                     Q    K    V\n",
        "        trg_ = self.attention(trg, src, src, src_mask)\n",
        "        # trg_: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        trg_ = self.dropout(trg_)\n",
        "\n",
        "        # residual connection and layer norm\n",
        "        trg  = self.attention_norm(trg + trg_)\n",
        "        # trg: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        trg_ = self.feedforward(trg)\n",
        "        # trg_: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        trg = self.dropout(trg)\n",
        "\n",
        "        # residual and layer norm\n",
        "        trg  = self.feedforward_norm(trg + trg_)\n",
        "        # trg: [batch_size, trg_len, emb_dim]\n",
        "\n",
        "        return trg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B7SgrOYNkV3"
      },
      "source": [
        "## The `transformer` Model\n",
        "\n",
        "The `transformer` model encapsulates the encoder and decoder and handles the creation of the source and target masks.\n",
        "\n",
        "The source mask, as described above, masks out `<pad>` tokens: the mask is 0 where the token is  a `<pad>` token and 1 otherwise. The mask is reshaped so that it can be broadcast to tensors of shape **[batch_size, n_heads, seq_len, seq_len]** which appear in the multi-head attention calculation.\n",
        "The target mask also includes a mask for the `<pad>` tokens.\n",
        "\n",
        "Consider a target sequence $\\text{<sos>}, t_1,\\cdots, t_{k}, \\text{<eos>}$ of length $k+2$ constructed with tokens, $t_i$, from the target vocabulary and delimited by the special tokens $t_0 \\equiv \\text{<sos>}$ and $t_{k+1} \\equiv \\text{<eos>}$, the start-of-sequence and end-of-sequence tokens, respectively. During training, ideally, we would like to test the quality of all predicted sub-sequences *simultaneously*. For example, given sub-sequences $\\text{<sos>}$ and  $\\text{<sos>}, t_1$ we would like to check simultaneously the prediction $\\text{<sos>} \\rightarrow \\ell_1$ and the prediction $\\text{<sos>}, t_1 \\rightarrow \\ell_2$ and so on, where $\\ell_1$ and $\\ell_2$ are the model predictions and $t_1$ and $t_2$ are the true target tokens.\n",
        "\n",
        "In practice, the $\\ell_i$ are vectors of weights, called **logits**, of dimension equal to the size $|v_t|$ of the target vocabulary. The logits for each sub-sequence are converted into a discrete probability distribution\n",
        "\\begin{align}\n",
        "p_{l} & \\equiv p(t_l \\in v_t| \\boldsymbol{x}, \\boldsymbol{y}_{\\lt l}),\n",
        "\\end{align}\n",
        "over the target vocabulary $v_t$\n",
        "and an algorithm, e.g., greedy or beam search, uses the probability distribution $p_l$ to predict the next token $y_l$ for each sub-sequence.\n",
        "\n",
        "During training, the transformer works on an entire source-target sequence pair. Therefore, it can compute the logits for all sub-sequences simultaneously. This is achieved with a simple, clever, trick: the so-called **subsequent mask**, `trg_sub_mask`. This mask is created using  the function $\\texttt{torch.tril}$, which creates a lower diagonal square matrix where the elements above the diagonal are zero and the elements below the diagonal are unity. For example, for a target sequence $\\boldsymbol{t} = \\text{<sos>}, t_1, t_2, t_{3}, \\text{<eos>}$ comprising 5 tokens  the `trg_sub_mask` looks like this:\n",
        "\n",
        "$$\\begin{pmatrix}\n",
        "1 & 0 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 0 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 0 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 0\\\\\n",
        "1 & 1 & 1 & 1 & 1\\\\\n",
        "\\end{pmatrix}.$$\n",
        "\n",
        "When applied to the target sequence, $\\boldsymbol{t}$, the subsequent mask ensures that for every sub-sequence the model has access to the next token when the losses are computed but not before. For example, the first row of the subsequent mask is **[1, 0, 0, 0, 0]**. Therefore, given the source sequence, $\\boldsymbol{x}$, only the `<sos>` token of the target sequence is available for prediction of the second token. The second row of the subsequent mask is **[1, 1, 0, 0, 0]**. In this case, the target tokens `<eos>` and $t_1$ are available to predict the third token, and so on.  During training, the\n",
        "subsequent mask makes it possible to compute losses for each sub-sequence simultaneously,\n",
        "\\begin{align}\n",
        "  \\boldsymbol{x}, \\text{<sos>}  \\rightarrow \\ell_1 & \\rightarrow loss(\\ell_1, t_1),\\\\\n",
        "  \\boldsymbol{x}, \\text{<sos>}, t_1   \\rightarrow \\ell_2 &\\rightarrow loss(\\ell_2, t_2), \\\\\n",
        "        : & : \\\\\n",
        "  \\boldsymbol{x}, \\text{<sos>}, t_1,\\cdots, t_{k}   \\rightarrow \\ell_{k+1} &\\rightarrow loss(\\ell_{k+1}, \\text{<eos>}) .\n",
        "\\end{align}\n",
        "Notice that this requires the target sequence into the decoder be stripped of its end-of-sequence token while it is the start-of-sequence token that must be stripped from the original target sequence before the losses are computed.\n",
        "    \n",
        "In evaluation mode, the model is used autoregressively: the predicted output sequence is initialized to the token $\\text{<sos>}$, which, together with the source sequence $\\boldsymbol{x}$, is used to predict the logits $\\ell_1$. The latter are converted to a discrete probability distribution over the target vocabulary which is then used to predict token $y_1$; then the source sequence and updated predicted sequence $\\text{<sos>}, y_1$ is fed into the transformer to predict $y_2$ and so on until either the token $\\text{<eos>}$ is predicted or the maximum allowed target sequence length is reached, whichever comes first.\n",
        "\n",
        "The target mask is the logical and of the target pad and subsequent masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifek5BPqNkV4"
      },
      "outputs": [],
      "source": [
        "importlib.reload(tnm)\n",
        "\n",
        "class Transformer(mlp.Model):\n",
        "\n",
        "    def __init__(self, config,\n",
        "                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "                 debug=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if not tnm.config_complete(config):\n",
        "            pass\n",
        "\n",
        "        self.pad    = config('PAD_CODE')\n",
        "        self.sos    = config('SOS_CODE')\n",
        "        self.eos    = config('EOS_CODE')\n",
        "        self.device = device\n",
        "        self.debug  = debug\n",
        "        self.max_len= config('TRG_SEQ_LEN')\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            config('SRC_VOCAB_SIZE'),\n",
        "            config('SRC_SEQ_LEN'),\n",
        "            config('ENC_EMB_DIM'),\n",
        "            config('ENC_LAYERS'),\n",
        "            config('ENC_HEADS'),\n",
        "            config('ENC_FF_DIM'),\n",
        "            config('ENC_DROPOUT'),\n",
        "            device)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            config('TRG_VOCAB_SIZE'),\n",
        "            config('TRG_SEQ_LEN'),\n",
        "            config('DEC_EMB_DIM'),\n",
        "            config('DEC_LAYERS'),\n",
        "            config('DEC_HEADS'),\n",
        "            config('DEC_FF_DIM'),\n",
        "            config('DEC_DROPOUT'),\n",
        "            device)\n",
        "\n",
        "        # initialize weights\n",
        "        if hasattr(self, 'weight') and self.weight.dim() > 1:\n",
        "            nn.init.xavier_uniform_(self.weight.data)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        src_mask = (src != self.pad).unsqueeze(1).unsqueeze(2)\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        # trg: [batch size, trg len]\n",
        "\n",
        "        _, trg_len = trg.shape\n",
        "\n",
        "        trg_pad_mask = (trg != self.pad).unsqueeze(1).unsqueeze(2)\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len),\n",
        "                                             device=self.device)).bool()\n",
        "        # trg_sub_mask: [trg_len, trg_len]\n",
        "\n",
        "        # logical AND of the two masks\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        if self.debug:\n",
        "            print('BEGIN')\n",
        "            print(' model - src.shape(on input):', src.shape)\n",
        "            print('       - ', src[:2])\n",
        "            print(' model - trg.shape(on input):', trg.shape)\n",
        "            print('       - ', trg[:2])\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        src      = self.encoder(src, src_mask)\n",
        "        # src: [batch_size, src_len, emb_dim]\n",
        "        if self.debug:\n",
        "            print(' model - src.shape(after encoder):', src.shape)\n",
        "            print('       - ', src[:2])\n",
        "\n",
        "        # the decoder will encode the target sequences\n",
        "        # before applying the attention layers.\n",
        "        logits   = self.decoder(trg, src, trg_mask, src_mask)\n",
        "        # output: [batch_size, trg_len, trg_vocab_size]\n",
        "\n",
        "        if self.debug:\n",
        "            print(' model - logits.shape(on exit):', logits.shape)\n",
        "            print('       - ', logits[:2])\n",
        "            print('END')\n",
        "        return logits\n",
        "\n",
        "    # see section \"Using the Model\" below\n",
        "    # -----------------------------------\n",
        "    def translate(self, src):\n",
        "        if src.ndim != 2:\n",
        "            sys.exit(f'''\n",
        "    translate: src must be of shape [batch_size, src_seq_len] with\n",
        "    batch_size = 1.\n",
        "            ''')\n",
        "\n",
        "        pad = self.pad\n",
        "        sos = self.sos\n",
        "        eos = self.eos\n",
        "        max_len= self.max_len\n",
        "        device = self.device\n",
        "\n",
        "        def execute(trg, src_, src_mask, top_k=3):\n",
        "            # trg:  list of integer codes\n",
        "            # src_: [batch_size, src_seq_len, src_vocab_size], batch_size = 1\n",
        "            if src_.ndim != 3:\n",
        "                sys.exit(f'''\n",
        "    translate.execute: src_ must be of shape [batch_size, src_seq_len, src_vocab_size] with\n",
        "    batch_size = 1.\n",
        "            ''')\n",
        "\n",
        "            trg_ = torch.tensor(trg).unsqueeze(0).to(device)\n",
        "            # trg_: [batch_size, trg_seq_len], batch_size = 1\n",
        "\n",
        "            trg_mask = self.make_trg_mask(trg_)\n",
        "            # trg_mask: [batch_size, 1, 1, trg_seq_len]\n",
        "\n",
        "            with torch.no_grad(): # no need to compute gradients\n",
        "                # defining y0 = <sos>, given the current predicted sequence,\n",
        "                # trg_ = (y0,..yi) and the source sequence, src_, compute\n",
        "                # logit vectors of size trg_vocab_size logits for the next\n",
        "                # token for each sub-sequence, (y0), (y0, y1), (y0,...,yi).\n",
        "                #\n",
        "                logits = self.decoder(trg_, src_, trg_mask, src_mask)\n",
        "                # logits: [batch_size, trg_seq_len, trg_vocab_size]\n",
        "                #\n",
        "                # ...and convert the logits to probabilities by applying\n",
        "                # a softmax to the trg_vocab_size dimension (dim=-1, i.e.,\n",
        "                # horizontally) of the last series of logits (logits[:, -1, :])\n",
        "                # Note: logits[:, -1, :] is of shape [batch_size, trg_vocab_size]\n",
        "                logits_for_final_sub_sequence = logits[:, -1, :]\n",
        "                probs = torch.softmax(logits_for_final_sub_sequence, dim=-1)\n",
        "\n",
        "            # return the top_k token codes with the largest probabilities\n",
        "            token_probs, token_codes = torch.topk(probs, k=top_k)\n",
        "            token_probs = token_probs.t() # transpose: [trg_seq_len, top_k] => [top_k, trg_seq_len]\n",
        "            token_codes = token_codes.t()\n",
        "\n",
        "            return token_probs, token_codes\n",
        "\n",
        "        # -------------------------------------\n",
        "        # Start autoregressive translation\n",
        "        # -------------------------------------\n",
        "        self.eval()\n",
        "\n",
        "        src = src.to(device) # add batch dimension\n",
        "        # src: [batch_size, src_seq_len], batch_size = 1\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        # src_mask: [batch_size, 1, 1, src_seq_len]\n",
        "\n",
        "        # encode (i.e., embed and analyze) source sequence\n",
        "        src_ = self.encoder(src, src_mask)\n",
        "        # src_: [batch_size, src_seq_len, emb_dim]\n",
        "\n",
        "        # initialize output sequence with the start-of-sequence token <sos>.\n",
        "        # the decoder takes in the encoded source sequence and for each\n",
        "        # current output sub-sequence computes weights for the next token.\n",
        "        # the logits for the next token of the last sub-sequence are converted\n",
        "        # to probabilities.\n",
        "        #\n",
        "        # using a greedy strategy, the most probable token is chosen as the\n",
        "        # next token, which is appended to the current output sequence.\n",
        "        # the algorithm repeats and stops when either the <eos> token is\n",
        "        # predicted or the maximum output sequence is reached.\n",
        "        trg = [sos]\n",
        "        for i in range(max_len):\n",
        "            # Note: must pass encoded source (src_) to execute\n",
        "            probs, codes = execute(trg, src_, src_mask)\n",
        "\n",
        "            code = codes[0, -1] # pick most probable next token\n",
        "            if code == pad:\n",
        "                continue\n",
        "\n",
        "            trg.append(code)\n",
        "            if code == eos:\n",
        "                break\n",
        "\n",
        "        return trg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB4xmAIVNkV4"
      },
      "source": [
        "## Training the `transformer` Model\n",
        "\n",
        "Our model is miniscule compared with the transformer models used today, of which there are many variants, but our model is small enough to be trained on a single GPU in less than an hour.\n",
        "\n",
        "As noted in Section *Transformer Model* above, given the entire source sequence, $\\boldsymbol{x}$, and all target sub-sequences, the model predicts the next logits for every target sub-sequence simultaneously. Consider, again, a target sequence of size $k = 5$. Since we want the model to predict `<eos>`, we slice off the `<eos>` token from the end of the target sequence,\n",
        "\\begin{align}\n",
        "\\text{trg} &= [\\text{<sos>}, t_1, t_2, t_3, \\text{<eos>}]\\\\\n",
        "\\text{trg[:-1]} &= [\\text{<sos>}, t_1, t_2, t_3],\n",
        "\\end{align}\n",
        "where the $t_i$ denote target sequence tokens other than `<sos>` and `<eos>` and the sliced target is fed into the model, which predicts simultaneously the logit vectirs $\\ell_1,\\cdots, \\ell_4$ corresponding to each of the sub-sequences $[\\text{<sos>}]$, $[\\text{<sos>}, t_1]$, $[\\text{<sos>}, t_1, t_2]$, and $[\\text{<sos>}, t_1, t_2, t_3]$, respectively. Recall that the dimension of each logit vector $\\ell_i$ is the cardinality $|v_t|$ (size) of the target vocabulary.  The loss, $loss(\\ell_i, t_i)$, for each sub-sequence is computed using the data\n",
        "\\begin{align}\n",
        "\\text{model outputs} &= [\\ell_1, \\ell_2, \\ell_3, \\ell_4]\\\\\n",
        "\\text{trg[1:]} &= [t_1, t_2, t_3, \\text{<eos>}] ,\n",
        "\\end{align}\n",
        "that is, the logits and the original targer tensor with the `<sos>` token stripped away.\n",
        "\n",
        "### Loss function\n",
        "A transformer is an autoregressive multi-class classifier: every time it is invoked with a source sequence and the current predicted output sequence, the classifier computes a probability distribution over the target vocabulary for every token in the current predicted sequence. These probability distributions can be used to decide upon, that is, classifier, the next output token. Therefore, like all multi-class classifiers, a transformer is trained using the **cross entropy loss**. Given the data $(\\ell_i, t_i)$, the loss is given by\n",
        "\\begin{align}\n",
        "    loss(\\ell_{i, k}, t_i) & = - \\log p_k, \\quad\\text{ where } k \\equiv t_i, \\\\\n",
        "        p_k & = \\texttt{softmax}_k(\\ell_i) \\equiv \\frac{\\exp(\\ell_{i, k})}{\\sum_{j} \\exp(\\ell_{i,j})},\n",
        "\\end{align}\n",
        "and $\\ell_{i, k}$ denotes component $k$ of logit vector $\\ell_i$. The losses are averaged over the output sub-sequences and over the batch of sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTJs53_3NkV4"
      },
      "outputs": [],
      "source": [
        "def train(objective, optimizer,\n",
        "          train_loader, train_small_loader, val_loader,\n",
        "          config):\n",
        "\n",
        "    # ------------------------------------------\n",
        "    # enter training loop\n",
        "    # ------------------------------------------\n",
        "\n",
        "    # get configuration info\n",
        "    lossfile   = config('file/losses')\n",
        "    paramsfile = config('file/params')\n",
        "    monstep    = config('monitor_step')\n",
        "    delete     = config('delete')\n",
        "    frac       = config('frac')\n",
        "    niterations= config('n_iterations')\n",
        "    base_lr    = config('base_lr')\n",
        "\n",
        "    # instantiate object that saves average losses to\n",
        "    # a csv file for realtime monitoring\n",
        "\n",
        "    losswriter = mon.LossWriter(niterations,\n",
        "                                lossfile,\n",
        "                                step=monstep,\n",
        "                                delete=delete,\n",
        "                                frac=frac,\n",
        "                                model=objective.model,\n",
        "                                paramsfile=paramsfile)\n",
        "\n",
        "    # -----------------------------\n",
        "    # training loop\n",
        "    # -----------------------------\n",
        "\n",
        "    for ii, (src, trg) in enumerate(train_loader):\n",
        "\n",
        "        objective.train()\n",
        "\n",
        "        R = objective(src, trg)\n",
        "\n",
        "        optimizer.zero_grad()     # zero gradients\n",
        "\n",
        "        R.backward()              # compute gradients\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "        optimizer.step()          # make a single step in average loss\n",
        "\n",
        "        if (ii % monstep == 0) or (ii == niterations-1):\n",
        "\n",
        "            # set mode to evaluation so that training-specific\n",
        "            # operations such as dropout, etc., are disabled.\n",
        "            objective.eval()\n",
        "\n",
        "            src, trg = next(iter(train_small_loader))\n",
        "            t_loss   = objective(src, trg).item()\n",
        "\n",
        "            src, trg = next(iter(val_loader))\n",
        "            v_loss   = objective(src, trg).item()\n",
        "\n",
        "            # update loss file\n",
        "            losswriter(ii, t_loss, v_loss, base_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3tXx6bcNkV4",
        "outputId": "bbd603f0-2858-48c7-8d7d-3e854c54350e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    Encoder\n",
            "    -------\n",
            "      vocabulary size:               37\n",
            "      sequence length:               22\n",
            "      embedding dimension:           64\n",
            "      number of layers:               2\n",
            "      number of heads:                8\n",
            "      hidden dim. of FFN:           128\n",
            "        \n",
            "\n",
            "    Decoder\n",
            "    -------\n",
            "      vocabulary size:               30\n",
            "      sequence length:               85\n",
            "      embedding dimension:           64\n",
            "      number of layers:               2\n",
            "      number of heads:                8\n",
            "      hidden dim. of FFN:           128\n",
            "        \n",
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (tok_embedding): Embedding(37, 64)\n",
            "    (pos_embedding): Embedding(22, 64)\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x EncoderLayer(\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (feedforward): Feedforward(\n",
            "          (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
            "          (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feedforward_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (tok_embedding): Embedding(30, 64)\n",
            "    (pos_embedding): Embedding(85, 64)\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x DecoderLayer(\n",
            "        (self_attention): MultiHeadAttention(\n",
            "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention): MultiHeadAttention(\n",
            "          (linear_Q): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_K): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_V): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (linear_O): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (feedforward): Feedforward(\n",
            "          (linear_1): Linear(in_features=64, out_features=128, bias=True)\n",
            "          (linear_2): Linear(in_features=128, out_features=64, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feedforward_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (linear): Linear(in_features=64, out_features=30, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "The model has 180,518 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "importlib.reload(tnm)\n",
        "\n",
        "model = Transformer(config, device=DEVICE).to(DEVICE)\n",
        "print(model)\n",
        "print(f'The model has {mlp.number_of_parameters(model):,} trainable parameters')\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config('base_lr'))\n",
        "\n",
        "avgloss   = nn.CrossEntropyLoss(ignore_index=config('PAD_CODE'))\n",
        "\n",
        "# make a specialized objective from mlp.Objective\n",
        "class TNNObjective(mlp.Objective):\n",
        "    def __init__(self, model, avgloss):\n",
        "        super().__init__(model, avgloss)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        # x[batch_size, src_seq_len]\n",
        "        # t[batch_size, trg_seq_len]\n",
        "\n",
        "        # Slice off <eos> token from all targets in the batch.\n",
        "        # For every sub-sequence, the model computes a vector\n",
        "        # of logits of size trg_vocab_size. The calculations\n",
        "        # for all sub-sequences are done simultaneously.\n",
        "        t_in = t[:, :-1] # strip await <eos>\n",
        "        logits = model(x, t_in)\n",
        "        # logits[batch_size, trg_seq_len, trg_vocab_size]\n",
        "\n",
        "        # reshape logits by flattening first 2 dimensions for\n",
        "        # the subsequent loss calculations\n",
        "        trg_vocab_size = logits.shape[-1] # get target vocabulary size\n",
        "        logits_out = logits.reshape(-1, trg_vocab_size)\n",
        "        # logits_out[batch_size * tgt_seq_len, tgt_vocab_size]\n",
        "\n",
        "        # For each sub-sequence, the model predicts the logits\n",
        "        # l_1, l_2,...,l_k, l_k+1, which ultimately will yield\n",
        "        # predict the next token, and for the last sub-sequence\n",
        "        # the token <eos>.\n",
        "        # Before we compute losses, we need to strip away the\n",
        "        # <sos> token from the targets to arrive at, t_1, t_2,\n",
        "        # ..., <eos> for the target sequences.\n",
        "        t_out = t[:, 1:].reshape(-1) # strip away <sos> and flatten\n",
        "        # [batch_size * tgt_seq_len]\n",
        "        return avgloss(logits_out, t_out).mean()\n",
        "\n",
        "objective = TNNObjective(model, avgloss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSKnr-o3NkV4"
      },
      "outputs": [],
      "source": [
        "TRAIN = False\n",
        "\n",
        "if TRAIN:\n",
        "    train(objective, optimizer,\n",
        "          train_loader, train_loader_val, val_loader,\n",
        "          config)\n",
        "\n",
        "    monitor = mon.Monitor(config('file/losses'))\n",
        "    monitor.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFWbhNtNkV4"
      },
      "source": [
        "## Using the Model\n",
        "\n",
        "The test data are already tokenized, coded, and bracketed with the `<sos>` and `<eos>` codes. The translation steps, implemented in `model.translate`, are as follows:\n",
        "\n",
        "  1. convert the coded source tokens, `src`, to the tensor, `src_`, and add a batch dimension to it (at dimension 0) so that the source is of the correct shape, namely, `[batch_size, src_len]`, but with `batch_size = 1`;\n",
        "  1. create the source mask `src_mask` to mask out pad tokens;\n",
        "  1. feed the source `src_` and its mask `src_mask` into the encoder;\n",
        "  1. for the predicted output sequence, create a list initialized with the `<sos>` token;\n",
        "  1. *repeat* steps `A` to `E` below until the model predicts the `<eos>` token or the maximum output length is reached:\n",
        "     1. convert the current output list `trg` into the tensor `trg_` and add a batch dimension at dimension 0 so that like the source tensor, the output tensor will have the shape `[batch_size, src_len]` with `batch_size = 1`;\n",
        "     1. create the target mask `trg_mask` to mask out pad tokens;\n",
        "     1. feed the current output `trg_`, encoder output `src_`, and the source and target masks into the decoder;\n",
        "     1. get the predicted token from the decoder;\n",
        "     1. add the predicted token to the current output list;\n",
        "  1. convert the output sequence from codes to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFSdM2NvNkV5",
        "outputId": "f4fa4977-7bdc-4959-93b7-933af1237402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     749\taccuracy:    0.979\n",
            "Accuracy:    0.979 +/- 0.005\n"
          ]
        }
      ],
      "source": [
        "PRINT_MISTAKES = False\n",
        "\n",
        "# load best model\n",
        "model.load(config('file/params'))\n",
        "\n",
        "M = 0\n",
        "F = 0.0\n",
        "for i, (src, trg) in enumerate(test_loader):\n",
        "    # src: [batch_size, src_seq_len], batch_size = 1\n",
        "    # trg: [batch_size, trg_seq_len], batch_size = 1\n",
        "\n",
        "    # translate: src => out\n",
        "    out = model.translate(src)\n",
        "    # out: list of integer codes\n",
        "\n",
        "    # convert sequence of target codes to a string (skipping <sos>,\n",
        "    # <eos>, and <pad> tokens)\n",
        "    trg  = trg.squeeze() # get rid of batch dimension for stringify\n",
        "    trg_ = tnm.stringify(trg[1:-1], seqdata.trg_code2token)\n",
        "\n",
        "    # convert predicted sequence of target codes to a string\n",
        "    # (skipping <sos>, <eos>, and <pad> tokens)\n",
        "    out_ = tnm.stringify(out[1:-1], seqdata.trg_code2token)\n",
        "\n",
        "    # count how often we're right\n",
        "    if out_ == trg_:\n",
        "        M += 1\n",
        "        F = M / (i+1)\n",
        "    else:\n",
        "        if PRINT_MISTAKES:\n",
        "            print()\n",
        "            print(tgt_)\n",
        "            print()\n",
        "            print(out_)\n",
        "            print()\n",
        "            print('-'*91)\n",
        "\n",
        "    print(f'\\r{i:8d}\\taccuracy: {F:8.3f}', end='')\n",
        "\n",
        "N  = len(test_data)\n",
        "dF = np.sqrt(F*(1-F)/N)\n",
        "print()\n",
        "print(f'Accuracy: {F:8.3f} +/- {dF:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k__dswXhDOwK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}