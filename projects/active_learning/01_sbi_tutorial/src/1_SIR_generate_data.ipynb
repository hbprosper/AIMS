{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simulation-Based Inference SIR Model: Data Generation\n",
    "Created: May 23, 2022 Prosper and Prosper<br>\n",
    "Updated: Oct 30, 2023 HBP\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The goal of this project is to use simulation-based inference (a.k.a., likelihood-free inference) to make reliable inferences about the SIR model when applied to the classic English Boarding School data. \n",
    "\n",
    "### SIR Model\n",
    "\n",
    "This is perhaps the simplest model of an epidemic with 3 compartments: susceptible ($S$), infected ($I$), and removed ($R$).\n",
    "\n",
    "#### Variables\n",
    "\n",
    "\\begin{align*}\n",
    "    S(t) &= \\mbox{mean number of susceptible persons at time $t$,}\\\\\n",
    "    I(t) &= \\mbox{mean number of infected persons at time $t$,}\\\\\n",
    "    R(t) &= \\mbox{mean number of persons removed from the infected class at time $t$,}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "\\begin{align*}\n",
    "    \\alpha &= \\mbox{removal rate (due to recovery or mortality); so $1/\\alpha$ is the mean infectious period,}\\\\\n",
    "    \\beta &= \\mbox{transmission rate per infected person.}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The mean number of new infections per day at time $t$ is\n",
    "$\\beta S(t) I(t)$.\n",
    "\n",
    "#### Equations\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{dS}{dt} & = - \\beta S I,\\\\\n",
    "    \\frac{dI}{dt} & = - \\alpha I + \\beta S I ,\\\\\n",
    "    \\frac{dR}{dt} & = \\alpha I.\n",
    "\\end{align}\n",
    "\n",
    "### Statistical inference\n",
    "Epidemics are stochastic processes. Therefore, in principle, they can be modeled with a statistical model. Unfortunately, the statistical models are generally intractable and approximations are needed. In Ref.[1], it is shown how (frequentist) inferences can be performed with proven guarantees, namely **exact coverage** for **confidence sets** provided that one has access to an accurate simulator of the stochastic process. Crucially, knowledge of the underlying statistical model, and hence likelihood function, is not needed. \n",
    "\n",
    "In the context of the SIR model, one uses a continuous time Markov chain (CTMC) simulator $F_\\theta$ to generate an ensemble of synthetic epidemics in which each epidemic, $j$, is associated with a randomly chosen point $\\theta_j = (\\alpha_j, \\beta_j)$ in the parameter space $\\Theta$ of the SIR model. \n",
    "Given a test statistic $\\lambda({\\cal D}_j;  \\theta_j)$ of our choosing, where ${\\cal D}_i$ denotes a time-ordered sequence of *simulated* counts of infected school children, one uses the synthetic data to estimate a variety of quantities, such as\n",
    "\n",
    "$$P(\\theta) \\equiv \\mathbb{P}[\\lambda({\\cal D}_j; \\theta_j) \\le \\lambda(D; \\theta_j)],$$ \n",
    "\n",
    "where $D$ denotes the sequence of *observed* counts of infected children. \n",
    "\n",
    "\n",
    "### Test statistic $\\lambda({\\cal D}; \\theta)$\n",
    "\n",
    "For the SIR model, with parameters $\\theta \\equiv \\alpha, \\beta$, we'll use the **test statistic**\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda(D; \\theta) & = \\frac{1}{50} \\sqrt{\\frac{1}{N} \\sum_{n=1}^N \\frac{[x_n - I_n(\\theta)]^2}{I_n}} ,\n",
    "\\end{align}\n",
    "\n",
    "where $D = x_1,\\cdots,x_N$ are the observed infection counts and $I_n = I(t_n, \\theta)$ is the predicted mean infection count at time $t_n$ found by solving the SIR equations\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{dS}{dt} & = - \\beta S I ,\\\\\n",
    "    \\frac{dI}{dt} & = - \\alpha I + \\beta S I .\n",
    "\\end{align}\n",
    "\n",
    "The test statistic $\\lambda$ (which is large for $\\theta$ values that are *disfavored* by the data) will be used to test hypotheses about the true value of $\\theta$. \n",
    "\n",
    "The ODE model gives an approximate description of the evolution of the *mean* counts. (A more accurate model replaces the product $S I$ by the average $\\langle S I \\rangle$.) However, the fact that the ODE model is approximate is, in principle, not a problem. What matters in simulation-based inference is that the simulator $F_\\theta$ is an accurate model of the data generation mechanism so that we can construct an accurate approximation of the distribution of the test statistic $\\lambda$. \n",
    "\n",
    "### Critical region\n",
    "In classical hypothesis testing the goal is to  *reject* hypotheses,  where $\\alpha$, not to be confused with the parameter that appears in the SIR model,  is the probability to reject a true hypothesis. Rejecting a true hypothesis is obviously  a mistake; it is called a __Type 1 error__. The hypothesis $H_0: \\theta = \\theta_0$ is __rejected__ if the test statistic falls in the __critical region__ defined (in our case) by  $\\lambda > C_\\theta$, where $C_\\theta$ is called the __critical value__.  If $\\lambda_0$ falls in the complementary region $\\lambda \\le C_\\theta$ then one has __failed to reject the hypothesis__. Then, one may choose to act as if the hypothesis $H_0: \\theta = \\theta_0$ has been accepted.\n",
    "\n",
    "Clearly, we want to limit the chance of rejecting hypotheses that happen to be true, that is, we want to keep small the probability of a Type 1 error. The smaller the value of $\\alpha$ the more stringent the condition for rejecting the hypothesis $H_0: \\theta = \\theta_0$. \n",
    "\n",
    "In medical research, the hypothesis $H_0: \\theta = \\theta_0$ is usually the so-called __null__ or __no effect__ hypothesis, for example, a treatment has no effect. The Type 1 error rate in that field is usually chosen to be $\\alpha = 0.05$ (for purely historical reasons). Unfortunately, this is such a weak condition that many *no effect* hypotheses are falsely rejected leading many medical researchers to claim effects that invariably turn out to be marginal at best. Particle physicists are much more cautious about rejecting null hypotheses, that is, hypotheses stating that there is nothing new in the data. The null hypotheses are rejected with $\\alpha = 2.7 \\times 10^{-7}$. But even with such a stringent criterion, particle physicists are not immune from making the occasional false discovery claim.\n",
    "\n",
    "\n",
    "### Confidence set\n",
    "The set of $\\theta$ values associated with hypotheses that failed to be rejected is called a __confidence set__ with __confidence level__ $\\tau = 1 - \\alpha$. The confidence set depends on the observed values $\\lambda_0$ through the condition $\\lambda_0 \\le C_\\theta$. Since (for a given $\\theta$) $\\lambda_0$ is an instance of a random variable, a given confidence set is likewise an instance of a __random set__ in the sense that the confidence set can change with each repetition of the experiment, or, in our case, epidemic. We say that confidence sets have __exact coverage__ if over an ensemble of experiments, or epidemics in our case, the fraction of confidence sets that contain the true value of $\\theta$ *never* falls below the desired confidence level $1 - \\alpha$. Note that the true value of $\\theta$ does not have to be the same in each epidemic. \n",
    "\n",
    "Confidence sets that fall below the confidence level are said to __undercover__. Generally, we're happy if we have approximate coverage, that is, coverage that does not fall too far below the desired confidence level.\n",
    "However, if the confidence sets have coverage that is well below the confidence level this is considered unsatisfactory because the true confidence level is then much lower than the claimed level of $1 - \\alpha$.\n",
    " \n",
    "### Simulated epidemics\n",
    "\n",
    "The goal of this notebook is to simulate epidemics like the one that inflicted the English Boarding School, compute test statistics, and discrete random variables of the form \n",
    "\n",
    "$$Z = \\mathbb{I}[\\lambda(X^\\prime; \\theta ) \\le \\lambda_0(X; \\theta)],$$\n",
    "\n",
    "where $\\mathbb{I}(x) = 1 \\text{ if } x \\text{ is true and } 0 \\text{ otherwise}$, $X^\\prime$ are simulated data, and  $\\lambda_0$ is the \"observed\" value of $\\lambda$ associated with \"observed\" data $X$. In Ref.[1], the observed data are the actual observations $D$; in the ALFFI approach, we simulate the \"observed\" data.\n",
    "\n",
    "### References\n",
    "  1. Ann Lee *et al.*, https://arxiv.org/abs/2107.03920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# the standard modules for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to reload modules\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# update fonts\n",
    "FONTSIZE = 18\n",
    "font = {'family' : 'serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "\n",
    "# set usetex = False if LaTex is not \n",
    "# available on your system or if the \n",
    "# rendering is too slow\n",
    "mp.rc('text', usetex=True)\n",
    "\n",
    "# set a seed to ensure reproducibility\n",
    "seed = 128\n",
    "rnd  = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SIR data and generate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D           : [  3  25  75 227 296 258 236 192 126  71  28  11   7]\n",
      " I0          : 3\n",
      " O           : [  3  25  75 227 296 258 236 192 126  71  28  11   7]\n",
      " R0          : 0\n",
      " S0          : 763\n",
      " T           : [ 0  2  3  4  5  6  7  8  9 10 11 12 13]\n",
      " alpha0      : 0.465\n",
      " alpha_bins  : 16\n",
      " alpha_max   : 1.0\n",
      " alpha_min   : 0.0\n",
      " alpha_scale : 1.0\n",
      " beta0       : 0.00237\n",
      " beta_bins   : 16\n",
      " beta_max    : 0.7\n",
      " beta_min    : 0.2\n",
      " beta_scale  : 0.005\n",
      " model       : SIR\n",
      " scale       : 50\n",
      " tmax        : 14.0\n",
      " tmin        : 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from SIR_genutil import generate, Fsolve, SIRdata\n",
    "print(SIRdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load $(\\alpha, \\beta)$ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of entries: 110000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.556824</td>\n",
       "      <td>0.432547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.917183</td>\n",
       "      <td>0.617733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222595</td>\n",
       "      <td>0.684092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513685</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.533168</td>\n",
       "      <td>0.343659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha      beta\n",
       "0  0.556824  0.432547\n",
       "1  0.917183  0.617733\n",
       "2  0.222595  0.684092\n",
       "3  0.513685  0.231400\n",
       "4  0.533168  0.343659"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/SIR_alpha_beta_110k.csv.gz')\n",
    "# N: number of epidemics to generate\n",
    "N  = len(df)\n",
    "print('number of entries: %d' % N)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/SIR_traindata_800k.csv')[:N].drop(labels='Unnamed: 0', axis=1)\n",
    "# print(len(df), df.columns)\n",
    "# df.to_csv('../data/SIR_traindata_100k.csv.gz', index=False)\n",
    "# df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic epidemics\n",
    "We'll choose a uniform prior $\\pi_\\theta$ as our __proposal distribution__ over the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate 110000 epidemics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████                      | 39052/110000 [2:20:03<14:15, 82.98it/s]"
     ]
    }
   ],
   "source": [
    "# length of data set\n",
    "nD = len(SIRdata.D)\n",
    "\n",
    "print(f'generate {N:d} epidemics')\n",
    "\n",
    "# get randomly sampled parameters\n",
    "alpha   = df.alpha.to_numpy()\n",
    "beta    = df.beta.to_numpy()\n",
    "I_means = [0]*N\n",
    "I_counts= [0]*N\n",
    "\n",
    "for j in tqdm(range(N)):\n",
    "    \n",
    "    # compute expected (i.e., mean) number of infections for current \n",
    "    # parameter point by solving coupled ODEs.\n",
    "    # the parameters will be scaled to the correct values within Fsolve\n",
    "    soln = Fsolve(alpha[j], beta[j])\n",
    "    I_means[j] = list(soln.y[1])\n",
    "\n",
    "    # generate data for one epidemic\n",
    "    params  = (alpha[j], beta[j])\n",
    "    _, i, _ = generate(params, SIRdata)\n",
    "    \n",
    "    # make list of simulated counts the same length as D\n",
    "    I_counts[j] = list(i) + (nD-len(i))*[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute statistics\n",
    "  1.  Shuffle epidemics to simulate \"observed\" data associated with randomly selected parameter points.\n",
    "  1. For each parameter point $(\\alpha, \\beta)$, compute statistic $\\lambda_o$ for randomly selected \"observed\" counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statistic(i, I):\n",
    "    return np.sqrt(np.array([(d-f)**2/f for d, f in zip(i, I)]).mean()) / SIRdata.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 110000/110000 [00:02<00:00, 50314.97it/s]\n"
     ]
    }
   ],
   "source": [
    "li = np.zeros(N)\n",
    "lo = np.zeros(N)\n",
    "l0 = np.zeros(N)\n",
    "Zo = np.zeros(N)\n",
    "Z0 = np.zeros(N)\n",
    "\n",
    "ii = np.arange(0, N, 1)\n",
    "np.random.shuffle(ii)\n",
    "\n",
    "for j in tqdm(range(N)):\n",
    "\n",
    "    # compute test statistic for simulated data\n",
    "    li[j] = test_statistic(I_counts[j], I_means[j])\n",
    "    \n",
    "    # compute test statistic for simulated \"observed\" data\n",
    "    lo[j] = test_statistic(I_counts[ii[j]], I_means[j])\n",
    "    Zo[j] = (li[j] <= lo[j]).astype(int)\n",
    "    \n",
    "    # compute test statistic for observed data\n",
    "    l0[j] = test_statistic(SIRdata.D, I_means[j])\n",
    "    Z0[j] = (li[j] <= l0[j]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the number of significant places to shorten the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imeans = [[float(int(100*x))/100 for x in y] for y in I_means]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert lists to string representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [str(x) for x in I_counts]\n",
    "I = [str(x) for x in Imeans]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>li</th>\n",
       "      <th>lo</th>\n",
       "      <th>l0</th>\n",
       "      <th>Zo</th>\n",
       "      <th>Z0</th>\n",
       "      <th>i</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.556824</td>\n",
       "      <td>0.432547</td>\n",
       "      <td>0.015810</td>\n",
       "      <td>0.629072</td>\n",
       "      <td>0.068463</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 33, 61, 149, 208, 217, 181, 125, 85, 61, 3...</td>\n",
       "      <td>[3.0, 25.41, 66.24, 140.76, 212.17, 226.2, 190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.917183</td>\n",
       "      <td>0.617733</td>\n",
       "      <td>0.037291</td>\n",
       "      <td>0.150635</td>\n",
       "      <td>0.323215</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 19, 101, 180, 180, 125, 72, 38, 21, 7, 9, ...</td>\n",
       "      <td>[3.0, 46.43, 126.68, 188.65, 160.33, 101.22, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222595</td>\n",
       "      <td>0.684092</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.545157</td>\n",
       "      <td>0.178384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 271, 514, 499, 413, 325, 265, 201, 155, 12...</td>\n",
       "      <td>[3.0, 232.07, 521.39, 511.27, 423.76, 341.88, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.513685</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.098132</td>\n",
       "      <td>0.418606</td>\n",
       "      <td>0.630670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 6, 11, 23, 39, 63, 81, 97, 95, 98, 75, 59,...</td>\n",
       "      <td>[3.0, 6.22, 8.89, 12.59, 17.59, 24.14, 32.47, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.533168</td>\n",
       "      <td>0.343659</td>\n",
       "      <td>0.047897</td>\n",
       "      <td>0.071392</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[3, 8, 19, 30, 52, 97, 150, 151, 124, 125, 106...</td>\n",
       "      <td>[3.0, 13.87, 28.69, 55.72, 97.06, 143.04, 172....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      alpha      beta        li        lo        l0   Zo   Z0  \\\n",
       "0  0.556824  0.432547  0.015810  0.629072  0.068463  1.0  1.0   \n",
       "1  0.917183  0.617733  0.037291  0.150635  0.323215  1.0  1.0   \n",
       "2  0.222595  0.684092  0.022214  0.545157  0.178384  1.0  1.0   \n",
       "3  0.513685  0.231400  0.098132  0.418606  0.630670  1.0  1.0   \n",
       "4  0.533168  0.343659  0.047897  0.071392  0.198700  1.0  1.0   \n",
       "\n",
       "                                                   i  \\\n",
       "0  [3, 33, 61, 149, 208, 217, 181, 125, 85, 61, 3...   \n",
       "1  [3, 19, 101, 180, 180, 125, 72, 38, 21, 7, 9, ...   \n",
       "2  [3, 271, 514, 499, 413, 325, 265, 201, 155, 12...   \n",
       "3  [3, 6, 11, 23, 39, 63, 81, 97, 95, 98, 75, 59,...   \n",
       "4  [3, 8, 19, 30, 52, 97, 150, 151, 124, 125, 106...   \n",
       "\n",
       "                                                   I  \n",
       "0  [3.0, 25.41, 66.24, 140.76, 212.17, 226.2, 190...  \n",
       "1  [3.0, 46.43, 126.68, 188.65, 160.33, 101.22, 5...  \n",
       "2  [3.0, 232.07, 521.39, 511.27, 423.76, 341.88, ...  \n",
       "3  [3.0, 6.22, 8.89, 12.59, 17.59, 24.14, 32.47, ...  \n",
       "4  [3.0, 13.87, 28.69, 55.72, 97.06, 143.04, 172....  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'alpha': alpha, \n",
    "                   'beta': beta, \n",
    "                   'li': li, \n",
    "                   'lo': lo, \n",
    "                   'l0': l0, \n",
    "                   'Zo': Zo, \n",
    "                   'Z0': Z0, \n",
    "                   'i': i, \n",
    "                   'I': I})\n",
    "df.to_csv('../data/SIR_traindata_110k.csv.gz', index=False, compression='gzip')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
